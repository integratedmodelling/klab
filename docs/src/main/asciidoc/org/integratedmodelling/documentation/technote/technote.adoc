= k.LAB: a semantic web platform for science
Ferdinando Villa <ferdinando.villa@bc3research.org>
Technical note 2021-02-20
:url-repo: https://docs.integratedmodelling.org/technote.html
:doctype: article
:description: High-level technical description of k.LAB for technical partners
:kl: k.LAB
:kmod: k.Modeler
:kact: k.Actors
:keng: k.LAB Engine
:knod: k.LAB Node
:kim: k.IM
:ked: k.LAB Resource Editor
:pex: Project Explorer
:kex: k.LAB Explorer
:encoding: utf-8
:lang: en
:title-page:
:toc: left
:toclevels: 5
:sectnums:
:sectnumlevels: 5
:numbered:
:experimental:
:reproducible:
:icons: font
:listing-caption: Listing
:sectnums:
:autofit-option:
:mdash: &#8212;
:language: asciidoc
:title-logo-image: image:resources_handling/imgs/KLAB_LOGO.png[align=center]
:source-highlighter: highlightjs
:highlightjs-languages: kim, java
:highlightjs-theme: klab
:stem:

<<<

*Integrated modeling* is a practice meant to maximize the value of scientific information by ensuring its  _modularity_, _reusability_, _interoperability_ and _traceability_ throughout the scientific process. The k.LAB software, discussed here, is a full-stack solution for integrated modelling, supporting the production, curation, linking and deployment of scientific artifacts such as datasets, data services, modular model components and distributed computational services. The purpose of k.LAB is to ensure -- by _design_ rather than intention -- that the pool of such artifacts constitutes a seamless _knowledge commons_, readily actionable (by humans or machines) through a full realization of the _linked data_ paradigm augmented with semantics and powered by artificial intelligence. This design enables automation of a wide range of modeling tasks that were previously only performable by experts and on an ad-hoc basis.

The {kl} platform directly addresses the four FAIR goals (Findable, Accessible, Interoperable and Reusable), introducing innovation particularly in the practice of **semantic annotation**, which is reinvented as a modern, expressive approach meant to ease adoption by both providers and users. To the four dimensions in FAIR, {kl} adds a _reactivity_ dimension, in line with the original vision of a semantic web: this dimension enables knowledge to also be _deployed_ in an _"internet of observations"_, creating _live_ artifacts that can interact, improve and evolve as new information appears on the network.

The central service in the {kl} modeling API wraps the _resolution algorithm_, which receives as input a logical query of the form "observe _concept_ in _context_" (e.g., "observe _change in land cover type_ in _Colombia, 2015-2020_", only slightly paraphrased from {kl}'s near-natural language query formalism) and, in response, assembles, documents, initializes and runs a computation (called a _dataflow_) that produces the  *observation* of the concept that best fits the context, based on the integration of data and model components available in the distributed k.LAB network. The observations output by the API request, along with the dataflow assembled to generate them, are themselves scientific artifacts -- automatically augmented with provenance records and user-readable documentation -- that can be exported and curated as needed. 

Artificial intelligence, driven by both semantics (_machine reasoning_) and the analysis of previous outcomes (_machine learning_), satisfies the request using a shared, communally owned and curated knowledge base (the _worldview_, a set of ontologies) and the resource pool available at any given moment on the k.LAB network, by ranking, selecting, adapting, and connecting data and model components made available by independent and uncoordinated providers. We refer to the process of building the computational strategy that "observes" a concept in a context as _resolution_, and to its execution to produce the desired result as _contextualization_, although we also refer to the combination of both processes when we discuss the "resolution service" provided by the API.

This document describes the main principles and architecture of {kl}. More detailed reference documentation for {kl} is in development and will be referenced in this document where available.

## Architecture of the k.LAB platform

The open source k.LAB software stack includes client and server components that support the creation, maintenance and use of a distributed _semantic web platform_ where scientific information can be stored, published, curated and connected. The software is licensed through the Affero General Public License (AGPL) v.3.0; the core components are available as a single project in the https://github.com/integratedmodelling/klab[{kl} git repository].

### The software stack

* *Server* components are deployed by certified _partners_ to publish resources and semantic content (*{knod}*) and/or to provide modeling services and applications (*{keng}*) to online users. Published resources can include both static data and dynamic computations, both of which may be hosted in source form at the node or linked to external data (e.g. WCS, WFS, OpenDAP) or computational services (e.g. OpenCPU). The k.Node software is deployed in containers that can be configured to host dedicated instances of Geoserver, PostgreSQL, Hyrax or other services; these are transparently managed through server adapters inside the node, virtually eliminating the need for specific training on those components for node administrators.
* *Client* components are used by contributors to develop, validate and publish resources and semantic content (*k.Modeler*, an Integrated Development Environment (IDE) for semantic modeling), and by end users (*k.Explorer*, a web-based application environment) to access modeling services and specialized applications built for the platform and delivered through the web.

Additional server components serve specific needs on the {kl} network and are of less common application in partner sites. Among them the following are noteworthy:

* The _hub_ server, *k.Hub*, manages authentication and organizes node access for authenticated engines. The Integrated Modeling Partnership manages a set of nodes and a main hub, and releases site certificates that enable nodes to be connected to form the platform. Partners that need to manage users locally may also deploy and connect a hub, although this is normally only convenient in large deployments.
* A _semantic server_ collects and indexes the semantic knowledge from the worldview and all public projects, constantly compiling and revising documentation and use cases to assist users in the semantic annotation process. Users can look up annotations made by others and access hyperlinked, evolving descriptions of each concept and predicate. The semantic server can be connected to the {kmod} editor to provide inline logical validation of logical expressions in models being developed, and a suggestion service that can find and propose comparisons with use cases extracted from peer-reviewed public projects. Through the use of specialized metadata inserted in {kim} source files, the server can be integrated with the editors so that assistance is available directly, to ease the development of semantic content as much as possible. The semantic server is in development and is not available to the general public yet. 

Other, less critical server components are in development and are not discussed here. Among these, a statistics server collects anonymized information from successful and unsuccessful resolutions and processes them using machine learning techniques to improve the resolution algorithm.

### The {kl} logical layers

The set of active, connected nodes and engines at any given time forms what can be seen collectively as a distributed container, where scientific knowledge is found in **three layers** handling information at increasing levels of abstraction: the _resources_, _semantic_ and _reactivity_ layers. The first can be seen as a data curation platform based on modern linked data concepts, using a generalized and flexible data model. Semantic and reactive content for the platform is developed in the respective layers using two specialized languages, _{kim}_ for semantic resources and _{kact}_ for reactive behaviors and applications. The modeler IDE (_{kmod}_) provides drag-and-drop interfaces to build resources and a specialized editing and debugging environment for {kl} projects, supporting both {kim} and {kact} development.

* The *resource* layer provides a _protocol_ for conventional data and computational resources or services to interoperate at the data level, matching identifiers, data types and metadata through a uniform API. Nodes and client applications include interfaces to manage development and submission of knowledge to the resources layer, to be published and curated either on-site or through hosting providers with full control of licensing and access. 
* The *semantic* layer provides a _language_ that enables annotating resources through semantically explicit logical expressions, ensuring findability, interoperability and accessibility through purely logical queries, validating consistency and producing mediation strategies through machine reasoning and logical inference. The semantic layer uses the **{kim}** language to specify semantic knowledge (compatible with W3-endorsed https://www.w3.org/TR/owl-guide/[OWL 2]) and models; these specifications, collected into namespaces and projects, can be deployed to {knod}s for the {kl} inference engines to find, rank and use.
* The *reactivity* layer provides _behaviors_ for the scientific artifacts produced by running queries in the semantic layer, effectively turning observations into software agents. Such reactive observations  can generate and react to events either locally (within the same session) or remotely. The reactivity layer enables distributed agent-based simulations and computations that automatically adapt to changing conditions or states. The **{kact}** language is used to define behaviors for the reactivity layer. As a special case, behaviors bound to users and sessions can be used to quickly develop specialized interactive applications that run on the {kex} platform, accessed through web browsers.

The separation of concerns and APIs in the three layers maximizes their value: for example, the resources layer can be seen through different semantics, therefore serving different purposes in different networks by reinterpreting it through the logical "lens" of a differently configured semantic layer.

### Accessing the system

The {kl} system can be accessed through client software (usually an application running in a browser within the {kex} web platform, or the {kl} integrated development environment (IDE), {kmod}) or through its API by software applications. Providers of content may use the IDE or, in the near future, a content provider web interface available on all {kl} nodes, including of course any nodes deployed at the provider's end. All users must be authenticated through a valid, secure certificate, which also establishes the semantic _worldview_ of reference and any user permissions for the certificate holder. Permissions may limit access to reserved or private content, which may be made available in public form or be linked to specific users or groups thereof by its owners.

Regular users:: Non-technical users of the {kl} platform normally interact with the system through an instance of {kex} exposed by a networked {kl} engine (or cluster thereof). The basic {kex} interface is usable as a generic search-and-compute interface and allows users to easily set their context of interest to locations and times of interest. Queries are cached and suggestions are given based on the user's groups and previous queries, providing an experience similar to modern search platforms. As {kex} can be used as an application development platform (see further in this document), specific applications can be built on top of {kex} and given a specialized access URL. Such applications, like the recently deployed https://seea.un.org/content/aries-for-seea[ARIES for SEEA], look and feel like typical interactive web applications and can be developed and deployed with very minimal effort to assist specific classes of users.
Content providers and modelers:: The k.LAB engine, a server-side component, can also be run at the client side in a local configuration, so that new content can be developed and tested in a sandboxed environment before publishing, with full access to public resources. Such client use is supported and facilitated by a small, downloadable https://integratedmodelling.org/get_started[control center application] that removes the complexities linked to installing, upgrading, starting and stopping the engine or the {kmod} IDE. The IDE remains, at the time of this writing, the endorsed toolkit to prepare both semantic and non-semantic content for distribution and publish it to the network. In the near future, more direct pathways will be enabled so that data contributors can also provide content (particularly datasets) through less technical, web-based interfaces.
Applications and software:: The {kl} system provides a stable API for all its server components, more notably the authenticating hub, the nodes and the engines. This API is used by all the {kl} client software but can be used independently to enact a "modeling as a service" paradigm whose primary service provided is the resolution algorithm. At the time of this writing, the API is mostly used through {kl}'s own client software, but ongoing projects and collaborations point to a more widespread integration of {kl} API services within foreign platforms and applications in the next months. In addition to direct use of REST endpoints, served by engine clusters operated by BC3 and partner institutions, client libraries for popular languages (Python, Javascript, R) will be made available based on demand to ease integration with existing applications.

In addition to uploading content to existing nodes, institutionals contributors can deploy {knod} software to deploy sites that contribute to the {kl} network while remaining fully in control of all distribution details. Nodes are deployed as containers that can be easily set up and authorized by certified partners. The k.LAB distributed paradigm supports and enforces a model where information remains under the ownership of its authoritative sources while maximizing its availability and interoperability, compatibly with both public and commercial services, thanks to careful attribution of ownership and to state-of-the-art encryption, access control and security.

## The resource layer

The resource layer contains or provides access to all "conventional", non-semantic informational assets available to {kl}: from raw datasets and bridges to external data services to algorithms expressed as mathematical equations or computable code. 

The aim of the resource layer is to present common conventions and a consistent API for k.LAB to access and manage pre-existing data, models and services of all kinds, as a first layer of interoperability. While the semantic layer specifies a _language_ for interoperability, the resource layer provides a generic _protocol_ that can be adapted to any existing data source or service as well as databases and external computations. Because resources have no semantics associated, it is possible to _reinterpret_ any resources through the desired semantics, enabling complete orthogonality between the resource layer and the semantic layer.

Importantly, in {kl}, _computations_, from simple equations to large and complex models, can also live in the resource layer. In fact, anything that takes inputs and produces outputs in numeric or other form, with no meaning explicitly attached beyond names and metadata, can be seen as a {kl} resource. All resources are identified by a Uniform Resource Name (URN) which can be resolved, through the {kl} API, to an informational record that contains all original metadata along with provenance information, history, and access permissions for the requesting user. Inputs, outputs and (in the case of resources that produce multiple objects) attributes will be similarly identified by a name and a data type.

Resources are not used directly by external clients in normal {kl} usage, although the resource API is open to authorized users and can be used as the base layer of a standard https://www.w3.org/TR/ldp/[_linked data_] platform. The resource URN is, instead, used in semantic models (see the next section) that in turn populate the search space for the semantically-driven resolution algorithm at the core of the system. A model that references a URN which the requesting user has no access to is automatically deactivated and does not participate in resolution, allowing the {kl} resolver to continue resolving through another authorized strategy. Where semantic assets have _semantics_ and _scale_, all resources have _data types_ (number, text, boolean, or probability distributions thereof) and a _geometry_, which defines the original representation of space and time, if any, in the resource. 

A resource URN is a fully specified identifier that uniquely references a resource in {kim} namespaces. It consists of 4 parts, separated by colons:

* A **node name** (the name of the node where the resource was originally published);
* A **catalog** (a logical space handled by the node, for example a domain such as hydrology, or a name describing a large-scale collection of data);
* A **namespace** (a secondary logical space within the catalog);
* A **resource identifier**.

The node name may consist of the reserved word `local` (identifying an unpublished local resource, see below) or `klab` to identify a "universal" resource with no network storage associated and handled directly by the engine, where the catalog part of the URN denotes a specific software adapter (e.g. `klab:random:` would introduce a URN pattern that produces various types of random data for testing, defined by the namespace and identifier). The core resource API, exposed by both {keng} and {knod}, provides a URN resolution service (URN -> resource metadata), standard Create/Read/Update/Delete (CRUD) operations on the resource layer, and the most important operation, _contextualization_, which takes as input a resource URN and a geometry specification and returns the data content of the resource adapted to the passed geometry. The contextualization return value is a flexible data structure (based on https://developers.google.com/protocol-buffers[Google Protobuf]) that allows efficient marshalling of zero or more objects, each with an internal structure that admits scalar or distributed values along grids or tessellations, conformant to the request geometry. The result, complete with metadata and provenance information but still devoid of semantics, is passed to the {kl} runtime to be turned into observations within the execution of a {kl} _dataflow_.

Not all resources occupy physical storage on {kl} nodes: in fact, {kl} extends the notion of the URN to encompass literals (e.g. `model 100 as geography:Elevation in m`, where `100` can be seen as a shorthand form of `klab:literals:values.parsed:number#value=100`) and specialized computational services which may simply serve as bridge to online services or computations (e.g. the urn `klab:osm:relations:park` would contextualize to all the parks stored as relations (polygons) in the https://www.openstreetmap.org[OpenStreetMap] service in the queried geometry). 

Diverse, extendible sourcing of information for resources is enabled through the use of _adapters_, i.e. software plug-ins that adapt a specific data or service format to the API. The adapter identifier and all adapter parameters are specified in the metadata associated to the URN and used to select the methods for contextualization, import, export and indexing. Adapters are made available as {kl} _components_, installable in {keng}s and {knod}s, and can be extended by developers using the Java API to support formats and services not yet available. External APIs (e.g. datacubes) can be supported by deploying a bridge adapter, as long as the original service provides all the information needed for {kl} to operate. To date, adapters for many file formats (CSV files, spatial rasters and vectors, NetCDF), protocols (WCS, WFS, OpenDAP, SDMX) and specialized services (OpenStreetMap, weather station data bridging to multiple databases and sources) are available and others (such as RDF/SPARQL) are in development. Other adapters enable specialized services, like scale-dependent selection of hierarchially organized datasets such as administrative regions or river basins. URN parameters can be added to the base URN to trigger specialized processing at the node's end, such as resolution-dependent simplification of polygons, selection of interpolation methods, or any other adapter-dependent option that will best suit the desired semantics.

### Lifecycle of {kl} resources

Resources start their life as _local_ within a user project, imported from files or through a resource editor integrated with the {kmod} client software. Such local resources go through a process of validation, meant to ensure that every need of the {kl} system can be satisfied in an integrated scenario of use: for example, spatial data must have proper projections and valid polygons throughout. When a local resource is accepted, it can be used inside the project that contains it or in any other project that shares the same local workspace, but is not visible to other users of the platform. Local resources may be sufficient for the needs of a specific, short-term project; yet, the natural lifecycle of a resource continues with _publication_, which makes it available across the {kl} network. 
 
Publication of a resource is conditional to further validation; no resources with incomplete metadata, licensing or ownership information is accepted by the software. Successful publication uploads the resource to the staging area of a chosen {kl} node, where it can be made available for general use and further edited in-place by its owner. Every edit of a published resource creates a new version of the resource and full history is kept. Published resources are independent of projects and obtain a unique URNs that never changes; the hosting {knod} may optimize their data content for faster serving and automatically mirror the resource to other nodes for increased availability. While public resources may be visibile, at the choice of their owner, only to selected users or groups of users, their URLs are universally recognized and can be used in {kim} models without the need for any registration or download, as long as the user is connected to the {kl} network.

The staging "tier" of the resource layer is, at the time of this writing, the only one enabled in {knod} software. It is envisioned that an iterative resource review process, operated by willing members of the community, will be used to promote resources to higher-ranking tiers, the level of which may affect the resolution algorithm, also incorporating user feedback and machine-learned statistics as resources get used as part of models. This process may eventually involve the attribution of a DOI to resources, resolved both through standard DOI proxy servers and directly by {kl}, enabling use of the DOI in place of the URN in semantic models. 

## The semantic layer: semantic modeling

Semantic modeling enables the _semantic annotation_ of non-semantic resources based on a shared _worldview_ (a logically organized knowledge base containing concepts and relationships). The linking of semantics to resource URNs is done in _models_, i.e. semantic annotations that specify the meaning associated with resources and, when applicable, with their inputs, outputs and attributes. As a non-semantic resource can represent both data and computations, {kl} treats data annotations and semantically annotated algorithms uniformly; as a result, we use the term _model_ to refer to both. The pool of models connected to {kl} sessions, organized into  _projects_ made available on {kl} nodes, constitutes the semantic layer, which is searched by the resolution algorithm to resolve a logical query to a result artifact. 

All semantic assets, from the knowledge base itself (concepts, relationships) to all semantically annotated content (data, algorithms) are specified in the **{kim} language**. While the underlying knowledge model for {kim} is the W3 standard OWL2 (to which all logical {kim} specifications can be translated), {kim}'s close resemblance to the structure of the English language makes it uncommonly readable:

[source,kim]
----
model occurrence of agriculture:Pollinator biology:Insect caused by earth:Weather
	observing
		earth:AtmosphericTemperature in Celsius named air_temperature,
		earth:SolarRadiation in J named solar_radiation
	set to [0.62 + 1.027 * air_temperature + 0.006 * solar_radiation];
----

In a departure from other ontology platforms, k.LAB admits, for the specification of semantics, logical expressions that combine predicates, operators and nouns in a fashion modeled on the grammar of the English language. For example, the {kim} statement `im:Net value of ecology:Pollination` (an _observable expression_, or _observable_ in short) contains a predicate (`im:Net`) and a semantic operator `value of` which affects the meaning of the process concept `ecology:Pollination` and transforms it into the concept representing its quantifiable value. This _linguistic_ articulation is key not only to the usability and parsimony of the underlying knowledge base, which can remain small and learnable thanks to the ability to combine and reuse terms and operators, but also to the functioning of the machine reasoning underlying the resolution algorithm, which can reason independently on the different logical dimensions of an observable and infer computations that would otherwise require specialized, _ad-hoc_ modeling. Lacking specific models for a complex observable, each logical dimension of it may be resolved to one or more models which handle that specific component, and the set of models, ranked for best fit to the context before selecting the most appropriate, can be used to assemble the best-case computation to produce the finished observation. The resulting _dataflow_ (algorithm) can, if wished, be saved as a non-semantic resource for future reference and reproducible reuse in {kl}. 

The specialized {kim} editor provided with {kmod} further facilitates the use and recognition of semantics by color-coding the fundamental classes of knowledge represented by concepts (blue for predicates, such as attributes, roles, realms or identities; green for quantifiable or categorizable qualities; red, green/yellow, yellow and brown respectively for processes, events, relationships and subjects) footnote:[See https://f1000research.com/articles/6-686[_Villa F, Balbi S, Athanasiadis IN and Caracciolo C. Semantics for interoperability of distributed data and models: Foundations for better-connected information_] for (slightly outdated) details on the phenomenological model underlying {kl}'s semantics.]. The editor is connected to the inference engine and further assists the modeler by checking the logical consistency of each observable as the user types and reporting any inconsistency as a syntax error. It is typical of k.LAB models to be very short, simple and easily readable. Every model, with few exceptions, resolves _one_ observable expression, with any required inputs stated merely as semantics; as a result, each model, by design, can be run and tested independently. For example, the model below 

[source,kim,linenumbers]
----
model occurrence of earth:Region with im:Still earth:PrecipitationVolume
	observing 
		earth:Upstream im:Area in m^2 named contributing_area,
		geography:Slope in degree_angle named slope
	set to [
		def sloperadians = Math.tan((slope*1.570796) / 90) 
	  	def twi = Math.log((contributing_area+1) / Math.tan((sloperadians+0.001)));
		return normalize(twi, -3.0, 30.0)
	];
----

requires observations of geographical slope and upstream drainage area to compute its output, a commonly used hydrological quantity (topographic wetness index) reinterpreted as a probability through the semantics of "occurrence of region with retained precipitation". None of the complex calculations required to compute the inputs needs to be part of the model, as their semantics (`earth:Upstream im:Area` and `geography:Slope`) is resolved at run time to the most appropriate model for the context when the primary observable is queried. The context can consist of a single point in space or of a gridded or polygon-based spatial coverage, without any modification to the model. If the context is temporally dynamic and the underlying state of a dependency (e.g. the slope) changes in time, the {kl} runtime will automatically notice the change and recompute the output, unless a specific model of `change in occurrence of earth:Region with im:Still earth:PrecipitationVolume` (a process affecting the quality after the `change in` operator) can be resolved in the context. When the model logics require that certain dependencies are satisfied in a specific way, scoping rules in {kim} can be used to ensure that specific models (or models for a specified subset) are chosen to satisfy the desired dependencies. It is also possible to use (libraries of) _non-semantic models_ to refer to specific computations whose semantics is deemed not worth exposing, ensuring linkage with conventionally used metrics without sacrificing modularity or requiring overly difficult semantic characterization. 

In many situations, models can be written independent of the specific spatial and temporal context in which they will be run, and often even in ways that are compatible with different interpretations of space and time. When that is not convenient, language constructs can be used to lock a model or namespace so that it is only applied to specified representations or ranges of extents and/or resolutions in both space and time, as well as to override the priorities in the resolution algorithm to handle any special need of the models or of the resources they use. Negotiation of inputs, outputs, data format, units or currencies, visualization and contextual validation are by default left to the k.LAB runtime. Writing models this way enforces discipline and maximizes clarity, readability and parsimony: contributors only write the core of the algorithm that leads to one specific observation, leaving every other aspect (including the selection and computation of any inputs) to the resolver and the k.LAB runtime.

### Semantic mediation and inference in support of modeling

In simple cases, the query "observe _observable_ in _context_" is answered by locating a model annotating a data source as an observation of the specified observable. For example, setting the context to a geographical region (e.g. a country's extent with a spatial grid model at 100m resolution and temporal context, e.g. the year 2010) and querying an observable such as `geography:Elevation in m` may retrieve, among others, the following model:

[source,kim]
----
model im.data:geography:morphology:dem90 as geography:Elevation in m;
----

which annotates a network-available resource specified by the URN `im.data:geography:morphology:dem90` as an observation of the `geography:Elevation` concept. The URN gives access to metadata including the original spatial and temporal coverage and resolution, through which the model, whose semantics is identical to the query's, can be ranked for match to the context. If the model is deemed to be the best match, the {kl} engine will translate it into a set of processing steps (in this case simply a resource retrieval operation plus any necessary mediation) and pass the resulting _dataflow_ to the runtime to compute and produce the resulting _observation_, in this case a raster map of elevation, with 100m resolution, reflecting the boundaries and time of the context. The dataflow will include any necessary reprojection, resampling, or unit transformation to match the query and the context. Other models may compete for the choice, made on the basis of criteria such as resolution and extent match, specificity, semantic match, and including criteria such as peer review results or usage feedback for the original data. If the chosen model only partially covers the context, additional models may contribute to its complete characterization, as long as their ranks are close enough.

Besides such simple and direct matches, machine reasoning backed by an observation-centered (as opposed to reality-centered) ontological framework can enable more sophisticated observation tasks that do not correspond to readily available annotations and are normally only possible through specialized, time-consuming work. In a straightforward example, attributes such as `im:Normalized` may be prepended to another observable to affect the result, where the attribute would be resolved to an independent model (`model im:Normalized using <normalization function>`), possibly restricted to certain classes of observables (e.g. `model im:Normalized of im:Quantity ...` to restrict its application to numerically quantifiable observables) and used to modify a straight observation of `geography:Elevation` if the normalized observable cannot be resolved directly. More interestingly, resolution strategies may cross inherency barriers to infer the best observation strategy when a direct match is not available. For example, a hypothetical query for `(ecology:AboveGround ecology:Biomass) of biology:Eucalyptus biology:Tree` footnote:[The `biology:Eucalyptus` species identity, used here for simplicity, would in reality be handled through a taxonomic authority: see the section _Authorities_ below for details.] operated in the same country context would refer, by virtue of the inherency operator `of`, to a quality (above-ground biomass) inherent to a particular subset (Eucalyptus) of the observations of a secondary subject (Tree) located in the primary context of the query (a geographical region). It would be resolved by the following strategy: 

1. Locate a model for the original observable, `(ecology:AboveGround ecology:Biomass) of biology:Eucalyptus biology:Tree`, that is compatible with the context of observation. If found, resolve using it. Otherwise
2. Locate a model of the inherent subject, `biology:Eucalyptus biology:Tree`; if found, accept it as the strategy to instantiate an observation for each eucalyptus tree in the region, so that a model of `(ecology:AboveGround ecology:Biomass)` can be later resolved in the context of each tree. If an "eucalyptus tree" model cannot be resolved
3. Locate a model capable of instantiating every `biology:Tree` in the region; if found, locate a classifier model capable of either 1) checking if the tree is eucalyptus or not (`model biology:Eucalyptus of biology:Tree`), or 2) attributing the abstract identity (`biology:Species`) of which `biology:Eucalyptus` is a subclass (`model biology:Species of biology:Tree`). Such a model would be applied to classify the tree observations, only keeping those that classify as eucalyptus. 
4. If eucalyptus trees are resolved successfully through either strategy (2) or (3), locate a model of `(ecology:AboveGround ecology:Biomass)` for each tree to compute the biomass in the context of each. If successful, insert a _dereifying_ operation to complete the observation, turning the "above ground biomass" values observed in the context of each tree into the quality "above ground biomass of eucalyptus tree" observed in the context region.

Similar reasoning strategies can be applied to a large set of situations, using semantic inference driven by the phenomenological understanding of the entities involved and the observation process applied to them. For example, a query for [.source.kim]`presence of biology:Tree` could be satisfied, when not resolvable directly, by a model of `(ecology:AboveGround ecology:Biomass) of biology:Tree` because biomass (a `im:Mass` in a higher-level ontology) is an _extensive_ property, therefore its non-zero value implies the existence of its inherent subject. The presence can be computed as a true/false value attributed to the context wherever the biomass of any tree is nonzero. In another commonly encountered use case, qualities that can only be correctly computed in specifically delineated contexts (for example hydrological qualities, such as "upstream area", which only produce correct results when computed in a correctly delineated river basin) can be automatically computed in arbitrary contexts by first looking up a model to delineate all the relevant contexts (river basins) intersecting the areas, then applying the necessary models to compute the qualities inherently to those, then re-distributing the values over the desired context. Such behavior can be automated simply on semantic grounds by defining a concept such as

[source,kim]
----
area ContributingArea
	is earth:Upstream im:Area within hydrology:RiverBasin;
----

or, more correctly, leaving the observable unconstrained and defining models as 

[source,kim]
----
model earth:Upstream im:Area within hydrology:RiverBasin
	....;
----

In both cases the 'within' operator mandates a RiverBasin context for the quality, which will trigger the distributed resolution process described whenever ContributingArea is queried in any context where river basins can be observed. The same considerations hold for more complex observables such as processes, which have the ability to affect the value of qualities through time and to generate events or other objects; these, in turn, can be the context for other qualities or processes. The ability to automatically negotiate mediations based on inherency and phenomenological reasoning multiplies the capability of connecting diverse models without error, offering integration possibilities orders of magnitude beyond those allowed by semantic matching alone. Such tasks require specific planning and significant technical expertise and time to perform in conventional ways.

Much of the power of {kl} comes from the fact that models pertaining to the different sides of a problem may be provided and shared by independent experts, with no need for any coordination besides adhering to the same worldview. Each model can serve multiple potential purposes and does not just _add to_, but rather _multiplies_ the value of other knowledge on the platform when interacting with it, just like words in natural language. The power of the resulting paradigm shift becomes obvious when the problem area addressed by modeling spans multiple disciplines, expertises and languages, emphasizing the importance of a collaboratively built and endorsed _worldview_. 

### The worldview

Both annotation and inference, as described above, require a common set of _ontologies_ that define the realm of knowledge that can be integrated and conform with the foundational principles of {kl}'s observational model. We refer to this set of ontologies as the _wordlview_, a set of {kim} projects that are automatically synchronized to all users that adopt it. A worldview is linked to each user profile and to the certificate that connects each {knod} to the {kl} network; only engines and nodes that adopt the same worldview as the user's are seen in a {kl} session.

As a worldview is meant to describe _observation_ of reality, not reality itself, it is naturally aware of _scale_; its semantics differentiates observables not only by phenomenological nature but also by the nature of the observation process applicable to them. For example, {kl} distinguishes _events_ from _processes_, a distinction that has no real epistemological rationale (and does not exist in ontologies such as http://www.obofoundry.org/ontology/bfo.html[BFO]) but is fundamental from an observational perspective, as events are _countable_ entities and therefore need to be instantiated, producing zero or more independent observations, before resolution, while only one instance of the same process may exist within the subject that provides a context for it. The range of scales of observation is key to the compatibility of worldviews: while a single worldview can easily address the wide range of problems that are "visible" at the scale of observation of a human observer, encompassing for example economic, ecological and social phenomena, it would be difficult to maintain meaning if that same worldview was also used to annotate problems at extremely small (e.g. quantum physics) or large (e.g. general relativity) observational scales. 

The development of a worldview is a large collaborative endeavor, whose success is essential to the full fruition of the {kl} paradigm. To date, only one worldview (the `im` worldview, for Integrated Modeling) is being developed, initially within the {kl} team with an extended group of collaborators. This worldview currently consists of _Tier 1_ namespaces, covering a set of disciplinary realms with only enough detail to enable {kl}'s current applications. As applications of {kl} grow, a process for the collaborative development, versioning and maintenance of the Tier 1 IM worldview will become an important area of emphasis. Tier 2 namespaces will be defined to specialize and add detail to the corresponding Tier 1 namespaces: for example, the Tier 1 `hydrology` namespace will be complemented by a project containing `hydrology.xxx` namespaces for each sub-area of hydrology needed by specialized applications. Such Tier 2 projects will be tied to user groups that each user can opt in through their user profile on the {kl} hub, so that those users can automatically access any projects and models that require Tier 2 concepts to be understood by the system. This modular approach will enable specific user groups to control the development of the needed terminology while remaining compatible with the core concepts in Tier 1, and allow a scaled and coordinated development of the knowledge base without overwhelming those users who don't need specialized detail. The semantic server described in the introduction will recognize the user groups and provide suggestions for annotation matching the chosen areas of expertise and level of detail.

#### Authorities

Providing semantics for identities such as taxonomic or chemical species presents a special challenge, as their number is virtually infinite: as a result, most commonly used ontologies (such as those in the http://www.obofoundry.org/[OBO foundry]) resort to providing _some_ of the identities most likely needed by the communities of reference, but it is impossible to address all use cases with full generality, and even importing specialized ontologies (such as http://www.obofoundry.org/ontology/chebi.html[CHEBI] for chemical identities) risks overwhelming the inference engine with too many (and still often not enough) concepts, or creating unnecessary incompatibility stemming from the usage of equivalent concepts from different ontologies. In {kl}, this problem is obviated through the introduction of _authorities_, a mechanism to interface with external vocabularies that enjoy broad community acceptance, fully integrated in the {kim} language. Such vocabularies are seen by contributors and users as externalized namespaces. An authoritative identity takes, in the {kim} language, the form `IUPAC:water`, easily distinguished from other concepts by its uppercase namespace identifier (a regular concept would have a lowercase namespace, e.g. `geography:Slope`). Its use in {kim} triggers validation of the concept identifier (`water`) using an online service tied to the authority (`IUPAC`), which is advertised by nodes in the k.LAB network. Upon successful validation, an identity concept is produced for the statement whose definition is identical and stable at all points of use. This mechanism allows externalizing large vocabularies (such as the IUPAC catalog of chemical species or the GBIF taxonomy identifiers) and structured specification conventions (such as the World Reference Base for soil types) that are validated and turned into stable, {kl}-aligned semantics at the moment of their use. Another advantage of many authorities is flexibility of specification: for example, `IUPAC:water` and `IUPAC:H2O` are valid identifiers that can be used in k.IM observables as written, and translate to the same concept (the chemical identity corresponding to water, encoded internally as the standard InChl key) using a IUPAC-endorsed catalog service provided by NIH. The k.LAB stack provides content contributors with assisted search interface and intelligent editor support with inline, "as-you-type" validation and documentation. The currently supported authorities include IUPAC, GBIF, the World Reference Base soil classification formalism, and the set of UN-endorsed statistical classifications provided through the FAO https://stats-class.fao.uniroma2.it/caliper[CALIPER] service (the latter in development at the time of this writing).

### Learning models

An important part of modeling is adapting a computation to known data, so that it can best reproduce a known output from a known set of inputs, in order to increase confidence in predicted results when the model is run with unknown inputs. The main use cases for this activity are _machine learning_, which iteratively _trains_ a statistical model until it produces the best fit to known data, and _model calibration_ or _data assimilation_, used to find the optimal parameterization of mechanistic models. No modeling platform would be complete without addressing these "learning" capabilities. In {kl}, model learning exploits the separation of the resource and semantic layer and the ability to find both inputs and outputs by resolving semantics. Models introduced by the keyword `learn` instead of `model` will resolve their outputs as well as their inputs, and produce, using a specialized function, a _computable resource_ with a specified URN, independent of semantics, containing the trained computation for future reuse. As an example, a minimal Bayesian suitability model to inform a land cover change model could be built using the following specification:

[source,kim]
----
learn landcover:LandCoverType
	observing
		@predictor distance to infrastructure:Highway,
		@predictor distance to earth:Waterway,
		@predictor distance to earth:Coastline,
		@predictor geography:Slope,
		@predictor geography:Elevation,
		@predictor count of demography:HumanIndividual,
		@predictor earth:AtmosphericTemperature in Celsius
	using im.weka.bayesnet(resource = luc.suitability);
----

The function call after the keyword `using` is a _contextualizer_ that invokes a learning process from the https://www.cs.waikato.ac.nz/ml/weka[WEKA] software, specifically a Bayesian learner. When run in a spatially distributed context, the above model will resolve both the output (land cover type) and all predictors in the context of execution, sample them to produce a training dataset, and pass the latter to Weka to build and train a Bayesian model, which is in turn used to produce the `luc.suitability` local resource (using the WEKA adapter) in the same project where the model is found. An interpolated map with the model's prediction, along with a report including all metrics of fit, is also produced to ease result evaluation. The trained Bayesian network can be modified and retrained as needed using WEKA and its integration with {kl}. When satisfactory, the trained model can be used for prediction through the URN of the trained resource:

[source,kim]
----
model luc.suitability as landcover:LandCoverType
	observing
		distance to infrastructure:Highway,
		distance to earth:Waterway,
		distance to earth:Coastline,
		geography:Slope,
		geography:Elevation,
		count of demography:HumanIndividual,
		earth:AtmosphericTemperature in Celsius;
----

The above model uses the trained Bayesian classifier to produce probabilistic predictions of land cover type. With probabilistic resources such as this, an uncertainty map can also be obtained by adding the uncertainty concept if desired:

[source,kim]
----
model luc.suitability as landcover:LandCoverType,
		uncertainty of landcover:LandCoverType
	observing
		....
----

Similar considerations apply to other learning algorithms such as the rest of the WEKA platform or others such as Google's TensorFlow. The resource containing the trained model will link its inputs by name and data type, and can be published to a node for remote execution by any users just like any other resource. Similar considerations apply to the prediction of qualities within countable entities (subjects, events, relationships) that are part of the context, training a classifier using each instance and its attributes as a training sample instead of sampling a distributed dataset like in the example above.

The problem of _calibration_ or _data assimilation_ of numerical models can be handled in the same fashion, by linking appropriate algorithms to {kl}. At the time of this writing, an interface to the open source http://www.openda.org/index.php[OpenDA] package is being investigated for future integration. 

### Sessions and outputs of contextualization

Within a {kl} session, a user or application sets a context and observes as many concepts as desired. Observations that were already made in the context automatically resolve any subsequent query for compatible concepts. At any time, the user or application can set or unset one or more _scenarios_ to affect the selection of models. A scenario in {kl} is simply a namespace whose contained models become "visible" to the system only when it is explicitly activated: when a scenario is active, its models take priority over any others to resolve their observables, potentially using other models to complete observations in case the scenario is only defined to cover a part of the context. Using scenarios, the environment within a context may be interactively defined to reflect specific hypotheses. In interactive use (for example with {kex}) it is possible to build observation sets that use different scenarios, incrementally defining a context that reflects any desired conditions.

A context always contains the complete history of observations made, including the metadata and provenance records for all resources and models used. As dataflows are resolved and contextualized, provenance records stored along with the knowledge will be extended with all logical steps followed to compute the corresponding observations, and remain available within the context to form a complete record of how the information in it has come into existence. All this information is available interactively to the user in graphical form when using a {kl} client, and becomes part of the set of downloadable artifacts accessible within a context. These include:

* A complete dataflow that includes all the processing steps and resources accessed to compute every observation in the context. The {kl} runtime uses a specialized language, k.DL, to encode dataflows in a concise and reusable way; the k.DL code can be visualized (as text or as a flowchart-like diagram) and saved to a resource to reproduce the computations as needed. When saving to a resource, the {kl} engine will compute the intersected spatial and temporal coverage of all resources and models involved, so that the dataflow can be saved along with the detailed geometry where the computations can be replicated.
* Complete provenance information for all the resource and models used in the context. The {kl} runtime adheres internally to the https://openprovenance.org/opm[Open Provenance Model (OPM)] conventions, which are central to the layout of the internal class structure. An API call to extract the OPM-compatible provenance graph for a context is expected in version 1.0.
* A tree of observations, each of which can be downloaded to the file formats supported by the configured adapters according to the spatial and temporal dimensions in the context. For example, an observations of a numerical or categorical quality (_state_) can be downloaded to a CSV file if scalar or distributed only in time, to a raster map (e.g. GeoTIFF or ArcGIS format) if spatially distributed on a grid, or to an archive file with a map per timestep if distributed in both space and time. Observations of subjects (e.g. the lakes in the context) can be downloaded to database files, including ESRI shapefiles when the objects have a spatial coverage.
* The user may request, in lieu of individual observations, _views_ that contextualize a specified concept and summarize the result in complex ways, such as tables or graphs. Such views also become part of the context along with all the observations made to compute them. These can be exported as spreadsheets or other appropriate formats. The table generation features in {kl} refer to observables using pure semantics, enable flexible specification of aggregations and allow users or modelers to build sophisticated and complex reports with very short specifications. Tables are prominently used, for example, in Natural Capital Accounting applications such as https://seea.un.org/content/aries-for-seea[ARIES for SEEA].
* As models are computed by the system, a user-readable, structured _report_ is generated and incorporated within the context. The documentation features in {kl} rely on a simple template language that can be associated to models in {kim} code and allows modelers to link documentation templates to events that are triggered during contextualization (for example, initialization or termination) and report sections such as introduction, methods, results and discussion. The {kmod} IDE contains specialized support for writing and organizing documentation in {kl} projects. By using the Markdown language supplemented with template directives, structured text can be inserted in the generated documentation along with figures, tables, cross-references and bibliographic citations. The {kl} engine incrementally assembles the report as new models are contextualized, producing a unified document that can be tailored to the context and to the actual results obtained using conditional template directives and context-aware text substitutions. This feature enables the production of very complete textual reports that can be downloaded as PDF through the clients or the API.

The set of outputs obtained and visualized during a {kl} session ensures the transparency and communicability of the results to a degree never seen in a modeling platform. In some situations, even the paths _not_ taken by the resolver can be documented, which may be relevant when multiple resources with close rankings are available in resolution. The possibility of producing _digitally signed artifacts_ including all of outputs, report, dataflow and full provenance graph, plus (if needed) verifying and documenting the provenance and the peer review status of all resources and models involved, opens the way to the production and the verification of _endorsed_ artifacts when the system is used to produce information from official institutional applications, or in situations when use of the result can have critical consequences in decision-making. 

### Extending the runtime system

The {kl} engines and nodes can be extended at the software level to provide new adapters, contextualizers, or other functionalities to support new integrations or resource types. A mechanism to produce _components_ that can be used as plug-ins uses well-defined and documented points of extension in the Java class structure, and is supported by Maven archetypes for convenient project setup, building and deployment. The design of the server components is highly modular, and each existing resource adapter, external package integration (such as the WEKA machine learning software) or functionality extension is written as a component that can be deployed to nodes and services. The contextualization runtime, which executes the resolved dataflows and can load them from a stored k.DL specification, can itself be swapped with an alternative execution runtime if wished, for example to support different runtime platforms (e.g. to run contextualizations on distributed file systems). The default runtime coming with {kl} is parallelized and multi-threaded, capable of handling concurrent sessions owned by different users and optimizing the use of RAM to enable large-scale simulations.

### Integrating external models

Integration of {kl} with existing models can proceed in two directions. By using the {kl} API from within an existing model, the inputs of the model can be satisfied using semantic resolution, streamlining and simplifying data access from a largely unmodified model. By contrast, deep integration of a model into the {kl} framework normally requires significant redesign, but can make the model and its components available to {kl} users and other models as part of the {kl} ecosystem, greatly enhancing its original value.

#### Integrating {kl} into existing models

In this integration configuration, the REST API of a {kl} engine (or cluster of engines) can be used, after authentication, from within an independent application to enable the use of the {kl} semantic network without integrating the application itself in {kl}. Applications that formerly loaded their outputs from the filesystem, relying on configuration files or interactive forms, would at this point simply define the geometry of interest and the semantics for their desired inputs. This paradigm does not make the application itself or its outputs available to {kl} users, and is therefore less valuable from an integration perspective, but it can constitute a first incentive to more productive integrations. At the time of this writing no language-specific client libraries have been written to ease the client use of {kl} from, e.g., Python or Javascript applications, but the direct use of the REST api remains possible.

#### Integrating existing models into {kl}

Integration of existing models so that they become part of the {kl} environment is possible in several ways. The preferred strategy is to break down the logical data flow inside a model into components that describe each individual concept within the model, then revise each of these components as independent models. From an interoperability perspective, this provides the greatest return, by ensuring the full integration of any internal feedbacks and sensitivity to changing boundary conditions. However, this approach also requires the most work to rethink each models internal logic, as most models have been written with specific conventions, if not even conditions of use, in mind that remain unwritten. This often mandates the generalization of the context of use of each model - for instance, generalizing a hydrologic model originally designed to run at an annual time scale to run on at more flexible time steps while remaining faithful to (time-agnostic) underlying physical processes to the degree possible. This may be difficult and time-consuming, particularly when the original implementation of the model is unclear, poorly documented, or logically inconsistent.

Preexisting models usually consist of highly connected networks of computations that are difficult to break into components to best fit an interoperable, semantic modeling paradigm. Yet, tightly defined and well-focused models can be used as "functions" when (1) their inputs and outputs are well-defined semantically, (2) data needs are clearly described, and (3) appropriate spatial/temporal scales for their use are provided. This is usually most convenient when their internal logic is complex and difficult to break up.

Three possible strategies to make pre-packaged models interoperable with k.LAB include:

1. Wrapping them into web services and connecting them to an API capable of mediating with k.LABs data transfer format. The model will be connected using the "remote" k.LAB adapter, which uses a REST API and can therefore be coupled to model services written in any language. This alternative requires little further work on the models themselves, but requires a "bridge" API for the host programming language to facilitate integration with the k.LAB interface. At the time of this writing bridge APIs exist only for Java, but those for other languages will be developed based on demand.
2. Creating a k.LAB contextualizer as an extension that gathers input from the k.LAB environment, passes it to the model for computation, and serves the outputs back. This does not require the mediation of a web service and thus entails more direct connections to the model code. The model may be connected at the code level, which is easiest in Java but can be supported by adapters for other languages.  Alternatively, the model may be run as an external application,  requiring no coding besides that needed to prepare inputs and gather outputs (this strategy is likely to be computationally inefficient, particularly for dynamic models that require independent runs over multiple time steps). Running as an external application may prove impossible when internal feedbacks must be connected to boundary conditions handled by the k.LAB environment, and while tempting because of the low development barrier, these kinds of solutions tend to have a limited useful life.
3. Isolating the core algorithms in the model and reimplementing them in code as contextualizers using the native k.LAB API. This middle-ground integration strategy neither reuses the original code as-is nor requires a full semantic annotation effort to fully integrate them. This approach is usually the easiest way to bring in existing logics without a major effort. As k.LAB takes care of I/O, data transformation and preparation, data flow, spatial and temporal addressing, and visualization, the rewrite usually only has to cover a small percentage of any original stand-alone model code, normally between 10 and 30%.

Overall, strategy 1 is the most generalizable solution (i.e., more bridge APIs would facilitate the integration of more external models with k.LAB). Strategy 3 is a practical solution when a smaller number of models are targeted for integration. Strategy 2 is the most _ad hoc_, with several key limitations; as such it can be seen as a generally less desirable strategy.

## The reactivity layer: behaviors and applications

The semantic modeling approach discussed so far is designed to construct simulated worlds, using the best available data and models, based on their logical description. The observations that compose these worlds can be construed as the outputs of the underlying modeling, and will incorporate any dynamic behavior that can be stated along with the logical description in {kim} models and contextualizers - typically, process models whose behavior is known in advance. While many phenomena can be described satisfactorily within this paradigm, others - namely, those where _events_ triggered by specific conditions cause modifications in the structure of the system - can not. Addressing these aspects of _agency_ and _reactivity_ is the purpose of the {kl} reactivity layer.

The reactivity layer contains a collection of _behaviors_, i.e. specifications of how any agent (the observations in a context, the context itself, or even the {kl} session or the user owning it) can react to conditions that come to pass during the course of contextualization. The reactivity layer is key to developing complex, distributed _agent-based models_ that are fully semantically aware, and allows modelers to build interactive visualizations and applications when the behavior is applied to a session. All behaviors take the form of code specified in the {kact} language, supported by the {kmod} IDE and used to define behaviors for observations, test cases, batch computations, UI components and interactive applications.

The {kact} language has a simple, minimal syntax that belies a complex and powerful model of execution. Both {kim} and {kact} draw their syntax from the English language; if the {kim} language is concerned with representing what observations _are_ and how they are computed, {kact} is concerned with representing how they _behave_. For this reason, the linguistic realm of {kim} is that of nouns, adjectives and adverbs, while {kact} deals mostly with _verbs_. Compared with {kim}, which is optimized to be usable at the simplest levels by modelers without programming experience, {kact} reads less like English than k.IM and is more suitable to experienced programmers. An annotated example is provided below, with no in-depth discussion, to give a flavor of the language:

[source,kactors]
----
behavior demo.restaurant
    "Invite a friend to dinner and if accepted, choose a restaurant in the context"

// the main action will be triggered when the behavior is loaded
action main:
	invite("friend@email.com"): "OK" -> choose({infrastructure:Restaurant}): reserve($)
	
action invite(friend):
	email("Hi, shall we go out for dinner tonight?", address=friend):
		answer -> sentiment.classify(answer, {im:Outcome}): (
						{im:Positive} -> email("Great", address=[answer.replyAddress]),  "OK"
					    {im:Negative} -> email("Sorry", address=[answer.replyAddress]), "NO")
					   
----

In the code above, two _actions_ are defined, each composed of one statement that calls other actions and specifies a chain of events triggered when each of them "responds" (_fires_). In action `main`, the verb `invite` is called, passing an email address as a parameter. The call, defined later in the code, results in an email being sent and its response being processed, eventually firing back a status code ("OK" or "NO") to the calling action. The OK code triggers the choice of a restaurant in the context and its booking when found. 

In {kact}'s concurrent mode of execution, actions may cause events (_fire_) zero or more times, and those events can be captured by the code that called the action using the `:` and `->` operators. When executing the code, the runtime starts each action and immediately moves on, without waiting for it to fire unless synchronous execution is forced. If the ':' operator follows the call, the actor running the behavior readies itself to process events fired by it, whenever they happen, which may be any time (or never) as long as the actor is "alive". The data associated with the event are matched to the expression that precedes the arrow operator `->`, and if the match succeeds the code following the operator is executed.

Behaviors written in {kact} can be, in the simplest cases, bound to the observations created by models using {kim} code:

[source,kim]
----
@bind(city.demo.behavior, select=[self.population > 100000])
model each klab:osm:point:city as infrastructure:City;
----

which will bind the `city.demo.behavior` behavior to any city whose population is higher than 100,000. Behaviors can also be bound to observations by actions in other behaviors, based on semantic type or other conditions, or directly from within code specified in {kim} models. 

In the forthcoming version 1.0 of {kl}, observations that are part of contexts in remote {kl} engines will be accessible by prepending the URL of a connected context to the identifier of each observation; this opens the door to _distributed real-time simulations_ whose agents can affect each other remotely. The paradigm of distributed, collaborative modeling enacted through the semantic layer can therefore, through the reactivity layer, extend to one where already initialized simulated worlds can interact with each other, building large-scale, multi-server simulations that can track events happening at each side. Institutions with expertise in tracking and predicting real-world phenomena of a particular category can make their digital "peers" available for other models to use. In the reference {kl} implementation, the actor facilities utilize open source technical solutions originally developed for the https://en.wikipedia.org/wiki/Internet_of_things[Internet of Things], capable of handling the functionalities described to build an "internet of observations" in support of real-time, better informed decision. 
 
### User-side applications

Within the {kl} runtime, the software "agents" capable of receiving a behavior are not only the observations built within sessions, but also the sessions themselves and the users that own them. This opens the door to the application of behaviors for purposes beyond the modeling of agents within simulations. In particularly, when a behavior is applied to a user session, the session can be seen as an _application_ whose actions are performed through the client software, and the consequences of which can trigger observations or other events. Coupled with the ability of {kact} to interact with the runtime and use semantics for queries, this feature enables a very intuitive way to build user applications in {kact}. The web client, {kex}, is fully equipped to respond to events triggered by actions by creating user interface components (such as buttons, text fields, lists etc.); users interacting with these components will "fire" events that are sent back to the {kact} runtime for processing. The resulting interactive application is typically very quick to build. For example, the following code

[source,kactors]
----
app example.ui.minimal
	"A simple demo of UI definition with k.Actors."
	description "This application demonstrates some basic UI widgets and interaction with the 
				 k.LAB runtime environment. An 'app' is a behavior applied to a k.LAB session." 
	style default with #{
    	font-size: '0.85em'
	}

@left
action main: 
	
	set outputs []

	%%% 
		**Markdown** and HTML text widget between matching percent markers (\%\%\%).  
		Write any *markdown* in this field to put text in the UI. The :scroll and
		:collapse attributes control the appearance.
	%%% :scroll :collapse

	/*
	 * Groups in parentheses become divisions in the UI and can be styled with layout
	 * attributes, titles and other properties through metadata 
	 */
	(
	  button("Set context to France and observe Elevation in it" #fr): 
	  		context(im.countries.france): 
			  	france -> france.observe({geography:Elevation}): (
					outputs.add($)
					fr.disable
				)
	  button("Observe vegetation C storage in the current context" #veg):  
	  		submit({ecology:Vegetation chemistry:Carbon im:Mass}): (
			  	outputs.add($)
				veg.disable
			)
	) :hbox :name "Sample observations (click to observe)"
	
	/*
	 * a final button enables downloading all the observations accumulated when pressing the 
	 * buttons above.
	 */
	button ("Maps" #mapdownload :tooltip "Download all observations as a zip file"): (
		mapdownload.waiting
		pack(outputs): ( 
			url -> (
				mapdownload.reset 
				download(url, filename="data.zip")
			)
			error -> mapdownload.error(:timeout 1000)
		)
	)
----

creates a demonstrational application that will show a buttons to make observations and collects the results in an array so that the corresponding data can be downloaded in one action. The UI will appear in {kex}. Using modular UI components also defined in {kact}, interfaces such as the code for the https://seea.un.org/content/aries-for-seea[ARIES for SEEA] application can be build by minimally trained programmers in a short time (the code for the ARIES for SEEA application at the time of this writing is only about 300 lines long), making sophisticated modeling services immediately available to users and decision makers with very little effort.

In addition to these usages, {kact} is used as a scripting language to automate repetitive tasks (for example to build global high-resolution maps describing a single observable, by computing it in multiple local contexts with fully customized model resolution in each) and to build test suites for all aspects of {kl}.

## Current status

The {kl} software stack is currently in version 0.11, and no feature-completion or API stability is guaranteed until version 1.0 is reached. According to funding and community uptake, this state is expected to be reachable in the period 2022 to 2023. The current status can be briefly summarized as follows: 

* The software can be considered at production levels for the functions that support applications such as the general {kl} explorer for the https://aries.integratedmodelling.org[ARIES project] and specialized apps like ARIES for SEEA. Visualization and reporting are at near-feature completion for current uses.
* Installable containers for nodes and engines are well-developed and used regularly, although few partner nodes besides the central team and the UN exist, and frequent upgrades are necessary.
* Feature completion vs. the planned set of features is at about 90%, enough for current applications but still needing work for full-scale agent-based modeling, real-time applications and other types of use.
* Resource adapters are available for most important data formats, services and protocols. Assisted user interfaces to contribute data and models are limited for now to the modeling environment {kmod}, which is functional but not suitable for non-technical users. More data submission methods and interfaces are to be developed in 2021 to support use by countries and institutions involved in ongoing projects.
* Besides an initial grant from the US National Science Foundation, {kl} has seen a limited but reliable funding stream for its development, with low- to mid-levels of financing but a relatively high stability. The current preference is for a partnership model rather than individual grants, as continuity and talent retention are more important to guarantee ultimate success than large investments and one-off funding.
* The REST API is currently optimized for applications and use "within" the system using its own clients based on {kex}: more discussion will be needed before a stable API specification, to be used independently, is published.

Technical inquiries on {kl} should be addressed to info@integratedmodelling.org.