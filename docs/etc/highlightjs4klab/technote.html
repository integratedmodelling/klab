<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Asciidoctor 2.0.15">
<meta name="description" content="High-level technical description of k.LAB for technical partners">
<meta name="author" content="Ferdinando Villa">
<title>k.LAB: a semantic web platform for science</title>
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:300,300italic,400,400italic,600,600italic%7CNoto+Serif:400,400italic,700,700italic%7CDroid+Sans+Mono:400,700">
<style>
/* Asciidoctor default stylesheet | MIT License | https://asciidoctor.org */
/* Uncomment @import statement to use as custom stylesheet */
/*@import "https://fonts.googleapis.com/css?family=Open+Sans:300,300italic,400,400italic,600,600italic%7CNoto+Serif:400,400italic,700,700italic%7CDroid+Sans+Mono:400,700";*/
article,aside,details,figcaption,figure,footer,header,hgroup,main,nav,section{display:block}
audio,video{display:inline-block}
audio:not([controls]){display:none;height:0}
html{font-family:sans-serif;-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%}
a{background:none}
a:focus{outline:thin dotted}
a:active,a:hover{outline:0}
h1{font-size:2em;margin:.67em 0}
abbr[title]{border-bottom:1px dotted}
b,strong{font-weight:bold}
dfn{font-style:italic}
hr{-moz-box-sizing:content-box;box-sizing:content-box;height:0}
mark{background:#ff0;color:#000}
code,kbd,pre,samp{font-family:monospace;font-size:1em}
pre{white-space:pre-wrap}
q{quotes:"\201C" "\201D" "\2018" "\2019"}
small{font-size:80%}
sub,sup{font-size:75%;line-height:0;position:relative;vertical-align:baseline}
sup{top:-.5em}
sub{bottom:-.25em}
img{border:0}
svg:not(:root){overflow:hidden}
figure{margin:0}
fieldset{border:1px solid silver;margin:0 2px;padding:.35em .625em .75em}
legend{border:0;padding:0}
button,input,select,textarea{font-family:inherit;font-size:100%;margin:0}
button,input{line-height:normal}
button,select{text-transform:none}
button,html input[type="button"],input[type="reset"],input[type="submit"]{-webkit-appearance:button;cursor:pointer}
button[disabled],html input[disabled]{cursor:default}
input[type="checkbox"],input[type="radio"]{box-sizing:border-box;padding:0}
button::-moz-focus-inner,input::-moz-focus-inner{border:0;padding:0}
textarea{overflow:auto;vertical-align:top}
table{border-collapse:collapse;border-spacing:0}
*,*::before,*::after{-moz-box-sizing:border-box;-webkit-box-sizing:border-box;box-sizing:border-box}
html,body{font-size:100%}
body{background:#fff;color:rgba(0,0,0,.8);padding:0;margin:0;font-family:"Noto Serif","DejaVu Serif",serif;font-weight:400;font-style:normal;line-height:1;position:relative;cursor:auto;tab-size:4;word-wrap:anywhere;-moz-osx-font-smoothing:grayscale;-webkit-font-smoothing:antialiased}
a:hover{cursor:pointer}
img,object,embed{max-width:100%;height:auto}
object,embed{height:100%}
img{-ms-interpolation-mode:bicubic}
.left{float:left!important}
.right{float:right!important}
.text-left{text-align:left!important}
.text-right{text-align:right!important}
.text-center{text-align:center!important}
.text-justify{text-align:justify!important}
.hide{display:none}
img,object,svg{display:inline-block;vertical-align:middle}
textarea{height:auto;min-height:50px}
select{width:100%}
.subheader,.admonitionblock td.content>.title,.audioblock>.title,.exampleblock>.title,.imageblock>.title,.listingblock>.title,.literalblock>.title,.stemblock>.title,.openblock>.title,.paragraph>.title,.quoteblock>.title,table.tableblock>.title,.verseblock>.title,.videoblock>.title,.dlist>.title,.olist>.title,.ulist>.title,.qlist>.title,.hdlist>.title{line-height:1.45;color:#7a2518;font-weight:400;margin-top:0;margin-bottom:.25em}
div,dl,dt,dd,ul,ol,li,h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6,pre,form,p,blockquote,th,td{margin:0;padding:0}
a{color:#2156a5;text-decoration:underline;line-height:inherit}
a:hover,a:focus{color:#1d4b8f}
a img{border:0}
p{font-family:inherit;font-weight:400;font-size:1em;line-height:1.6;margin-bottom:1.25em;text-rendering:optimizeLegibility}
p aside{font-size:.875em;line-height:1.35;font-style:italic}
h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6{font-family:"Open Sans","DejaVu Sans",sans-serif;font-weight:300;font-style:normal;color:#ba3925;text-rendering:optimizeLegibility;margin-top:1em;margin-bottom:.5em;line-height:1.0125em}
h1 small,h2 small,h3 small,#toctitle small,.sidebarblock>.content>.title small,h4 small,h5 small,h6 small{font-size:60%;color:#e99b8f;line-height:0}
h1{font-size:2.125em}
h2{font-size:1.6875em}
h3,#toctitle,.sidebarblock>.content>.title{font-size:1.375em}
h4,h5{font-size:1.125em}
h6{font-size:1em}
hr{border:solid #dddddf;border-width:1px 0 0;clear:both;margin:1.25em 0 1.1875em;height:0}
em,i{font-style:italic;line-height:inherit}
strong,b{font-weight:bold;line-height:inherit}
small{font-size:60%;line-height:inherit}
code{font-family:"Droid Sans Mono","DejaVu Sans Mono",monospace;font-weight:400;color:rgba(0,0,0,.9)}
ul,ol,dl{font-size:1em;line-height:1.6;margin-bottom:1.25em;list-style-position:outside;font-family:inherit}
ul,ol{margin-left:1.5em}
ul li ul,ul li ol{margin-left:1.25em;margin-bottom:0;font-size:1em}
ul.square li ul,ul.circle li ul,ul.disc li ul{list-style:inherit}
ul.square{list-style-type:square}
ul.circle{list-style-type:circle}
ul.disc{list-style-type:disc}
ol li ul,ol li ol{margin-left:1.25em;margin-bottom:0}
dl dt{margin-bottom:.3125em;font-weight:bold}
dl dd{margin-bottom:1.25em}
abbr,acronym{text-transform:uppercase;font-size:90%;color:rgba(0,0,0,.8);border-bottom:1px dotted #ddd;cursor:help}
abbr{text-transform:none}
blockquote{margin:0 0 1.25em;padding:.5625em 1.25em 0 1.1875em;border-left:1px solid #ddd}
blockquote,blockquote p{line-height:1.6;color:rgba(0,0,0,.85)}
@media screen and (min-width:768px){h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6{line-height:1.2}
h1{font-size:2.75em}
h2{font-size:2.3125em}
h3,#toctitle,.sidebarblock>.content>.title{font-size:1.6875em}
h4{font-size:1.4375em}}
table{background:#fff;margin-bottom:1.25em;border:solid 1px #dedede;word-wrap:normal}
table thead,table tfoot{background:#f7f8f7}
table thead tr th,table thead tr td,table tfoot tr th,table tfoot tr td{padding:.5em .625em .625em;font-size:inherit;color:rgba(0,0,0,.8);text-align:left}
table tr th,table tr td{padding:.5625em .625em;font-size:inherit;color:rgba(0,0,0,.8)}
table tr.even,table tr.alt{background:#f8f8f7}
table thead tr th,table tfoot tr th,table tbody tr td,table tr td,table tfoot tr td{line-height:1.6}
h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6{line-height:1.2;word-spacing:-.05em}
h1 strong,h2 strong,h3 strong,#toctitle strong,.sidebarblock>.content>.title strong,h4 strong,h5 strong,h6 strong{font-weight:400}
.center{margin-left:auto;margin-right:auto}
.stretch{width:100%}
.clearfix::before,.clearfix::after,.float-group::before,.float-group::after{content:" ";display:table}
.clearfix::after,.float-group::after{clear:both}
:not(pre).nobreak{word-wrap:normal}
:not(pre).nowrap{white-space:nowrap}
:not(pre).pre-wrap{white-space:pre-wrap}
:not(pre):not([class^=L])>code{font-size:.9375em;font-style:normal!important;letter-spacing:0;padding:.1em .5ex;word-spacing:-.15em;background:#f7f7f8;-webkit-border-radius:4px;border-radius:4px;line-height:1.45;text-rendering:optimizeSpeed}
pre{color:rgba(0,0,0,.9);font-family:"Droid Sans Mono","DejaVu Sans Mono",monospace;line-height:1.45;text-rendering:optimizeSpeed}
pre code,pre pre{color:inherit;font-size:inherit;line-height:inherit}
pre>code{display:block}
pre.nowrap,pre.nowrap pre{white-space:pre;word-wrap:normal}
em em{font-style:normal}
strong strong{font-weight:400}
.keyseq{color:rgba(51,51,51,.8)}
kbd{font-family:"Droid Sans Mono","DejaVu Sans Mono",monospace;display:inline-block;color:rgba(0,0,0,.8);font-size:.65em;line-height:1.45;background:#f7f7f7;border:1px solid #ccc;-webkit-border-radius:3px;border-radius:3px;-webkit-box-shadow:0 1px 0 rgba(0,0,0,.2),0 0 0 .1em white inset;box-shadow:0 1px 0 rgba(0,0,0,.2),0 0 0 .1em #fff inset;margin:0 .15em;padding:.2em .5em;vertical-align:middle;position:relative;top:-.1em;white-space:nowrap}
.keyseq kbd:first-child{margin-left:0}
.keyseq kbd:last-child{margin-right:0}
.menuseq,.menuref{color:#000}
.menuseq b:not(.caret),.menuref{font-weight:inherit}
.menuseq{word-spacing:-.02em}
.menuseq b.caret{font-size:1.25em;line-height:.8}
.menuseq i.caret{font-weight:bold;text-align:center;width:.45em}
b.button::before,b.button::after{position:relative;top:-1px;font-weight:400}
b.button::before{content:"[";padding:0 3px 0 2px}
b.button::after{content:"]";padding:0 2px 0 3px}
p a>code:hover{color:rgba(0,0,0,.9)}
#header,#content,#footnotes,#footer{width:100%;margin-left:auto;margin-right:auto;margin-top:0;margin-bottom:0;max-width:62.5em;*zoom:1;position:relative;padding-left:.9375em;padding-right:.9375em}
#header::before,#header::after,#content::before,#content::after,#footnotes::before,#footnotes::after,#footer::before,#footer::after{content:" ";display:table}
#header::after,#content::after,#footnotes::after,#footer::after{clear:both}
#content{margin-top:1.25em}
#content::before{content:none}
#header>h1:first-child{color:rgba(0,0,0,.85);margin-top:2.25rem;margin-bottom:0}
#header>h1:first-child+#toc{margin-top:8px;border-top:1px solid #dddddf}
#header>h1:only-child,body.toc2 #header>h1:nth-last-child(2){border-bottom:1px solid #dddddf;padding-bottom:8px}
#header .details{border-bottom:1px solid #dddddf;line-height:1.45;padding-top:.25em;padding-bottom:.25em;padding-left:.25em;color:rgba(0,0,0,.6);display:-ms-flexbox;display:-webkit-flex;display:flex;-ms-flex-flow:row wrap;-webkit-flex-flow:row wrap;flex-flow:row wrap}
#header .details span:first-child{margin-left:-.125em}
#header .details span.email a{color:rgba(0,0,0,.85)}
#header .details br{display:none}
#header .details br+span::before{content:"\00a0\2013\00a0"}
#header .details br+span.author::before{content:"\00a0\22c5\00a0";color:rgba(0,0,0,.85)}
#header .details br+span#revremark::before{content:"\00a0|\00a0"}
#header #revnumber{text-transform:capitalize}
#header #revnumber::after{content:"\00a0"}
#content>h1:first-child:not([class]){color:rgba(0,0,0,.85);border-bottom:1px solid #dddddf;padding-bottom:8px;margin-top:0;padding-top:1rem;margin-bottom:1.25rem}
#toc{border-bottom:1px solid #e7e7e9;padding-bottom:.5em}
#toc>ul{margin-left:.125em}
#toc ul.sectlevel0>li>a{font-style:italic}
#toc ul.sectlevel0 ul.sectlevel1{margin:.5em 0}
#toc ul{font-family:"Open Sans","DejaVu Sans",sans-serif;list-style-type:none}
#toc li{line-height:1.3334;margin-top:.3334em}
#toc a{text-decoration:none}
#toc a:active{text-decoration:underline}
#toctitle{color:#7a2518;font-size:1.2em}
@media screen and (min-width:768px){#toctitle{font-size:1.375em}
body.toc2{padding-left:15em;padding-right:0}
#toc.toc2{margin-top:0!important;background:#f8f8f7;position:fixed;width:15em;left:0;top:0;border-right:1px solid #e7e7e9;border-top-width:0!important;border-bottom-width:0!important;z-index:1000;padding:1.25em 1em;height:100%;overflow:auto}
#toc.toc2 #toctitle{margin-top:0;margin-bottom:.8rem;font-size:1.2em}
#toc.toc2>ul{font-size:.9em;margin-bottom:0}
#toc.toc2 ul ul{margin-left:0;padding-left:1em}
#toc.toc2 ul.sectlevel0 ul.sectlevel1{padding-left:0;margin-top:.5em;margin-bottom:.5em}
body.toc2.toc-right{padding-left:0;padding-right:15em}
body.toc2.toc-right #toc.toc2{border-right-width:0;border-left:1px solid #e7e7e9;left:auto;right:0}}
@media screen and (min-width:1280px){body.toc2{padding-left:20em;padding-right:0}
#toc.toc2{width:20em}
#toc.toc2 #toctitle{font-size:1.375em}
#toc.toc2>ul{font-size:.95em}
#toc.toc2 ul ul{padding-left:1.25em}
body.toc2.toc-right{padding-left:0;padding-right:20em}}
#content #toc{border-style:solid;border-width:1px;border-color:#e0e0dc;margin-bottom:1.25em;padding:1.25em;background:#f8f8f7;-webkit-border-radius:4px;border-radius:4px}
#content #toc>:first-child{margin-top:0}
#content #toc>:last-child{margin-bottom:0}
#footer{max-width:none;background:rgba(0,0,0,.8);padding:1.25em}
#footer-text{color:rgba(255,255,255,.8);line-height:1.44}
#content{margin-bottom:.625em}
.sect1{padding-bottom:.625em}
@media screen and (min-width:768px){#content{margin-bottom:1.25em}
.sect1{padding-bottom:1.25em}}
.sect1:last-child{padding-bottom:0}
.sect1+.sect1{border-top:1px solid #e7e7e9}
#content h1>a.anchor,h2>a.anchor,h3>a.anchor,#toctitle>a.anchor,.sidebarblock>.content>.title>a.anchor,h4>a.anchor,h5>a.anchor,h6>a.anchor{position:absolute;z-index:1001;width:1.5ex;margin-left:-1.5ex;display:block;text-decoration:none!important;visibility:hidden;text-align:center;font-weight:400}
#content h1>a.anchor::before,h2>a.anchor::before,h3>a.anchor::before,#toctitle>a.anchor::before,.sidebarblock>.content>.title>a.anchor::before,h4>a.anchor::before,h5>a.anchor::before,h6>a.anchor::before{content:"\00A7";font-size:.85em;display:block;padding-top:.1em}
#content h1:hover>a.anchor,#content h1>a.anchor:hover,h2:hover>a.anchor,h2>a.anchor:hover,h3:hover>a.anchor,#toctitle:hover>a.anchor,.sidebarblock>.content>.title:hover>a.anchor,h3>a.anchor:hover,#toctitle>a.anchor:hover,.sidebarblock>.content>.title>a.anchor:hover,h4:hover>a.anchor,h4>a.anchor:hover,h5:hover>a.anchor,h5>a.anchor:hover,h6:hover>a.anchor,h6>a.anchor:hover{visibility:visible}
#content h1>a.link,h2>a.link,h3>a.link,#toctitle>a.link,.sidebarblock>.content>.title>a.link,h4>a.link,h5>a.link,h6>a.link{color:#ba3925;text-decoration:none}
#content h1>a.link:hover,h2>a.link:hover,h3>a.link:hover,#toctitle>a.link:hover,.sidebarblock>.content>.title>a.link:hover,h4>a.link:hover,h5>a.link:hover,h6>a.link:hover{color:#a53221}
details,.audioblock,.imageblock,.literalblock,.listingblock,.stemblock,.videoblock{margin-bottom:1.25em}
details>summary:first-of-type{cursor:pointer;display:list-item;outline:none;margin-bottom:.75em}
.admonitionblock td.content>.title,.audioblock>.title,.exampleblock>.title,.imageblock>.title,.listingblock>.title,.literalblock>.title,.stemblock>.title,.openblock>.title,.paragraph>.title,.quoteblock>.title,table.tableblock>.title,.verseblock>.title,.videoblock>.title,.dlist>.title,.olist>.title,.ulist>.title,.qlist>.title,.hdlist>.title{text-rendering:optimizeLegibility;text-align:left;font-family:"Noto Serif","DejaVu Serif",serif;font-size:1rem;font-style:italic}
table.tableblock.fit-content>caption.title{white-space:nowrap;width:0}
.paragraph.lead>p,#preamble>.sectionbody>[class="paragraph"]:first-of-type p{font-size:1.21875em;line-height:1.6;color:rgba(0,0,0,.85)}
table.tableblock #preamble>.sectionbody>[class="paragraph"]:first-of-type p{font-size:inherit}
.admonitionblock>table{border-collapse:separate;border:0;background:none;width:100%}
.admonitionblock>table td.icon{text-align:center;width:80px}
.admonitionblock>table td.icon img{max-width:none}
.admonitionblock>table td.icon .title{font-weight:bold;font-family:"Open Sans","DejaVu Sans",sans-serif;text-transform:uppercase}
.admonitionblock>table td.content{padding-left:1.125em;padding-right:1.25em;border-left:1px solid #dddddf;color:rgba(0,0,0,.6);word-wrap:anywhere}
.admonitionblock>table td.content>:last-child>:last-child{margin-bottom:0}
.exampleblock>.content{border-style:solid;border-width:1px;border-color:#e6e6e6;margin-bottom:1.25em;padding:1.25em;background:#fff;-webkit-border-radius:4px;border-radius:4px}
.exampleblock>.content>:first-child{margin-top:0}
.exampleblock>.content>:last-child{margin-bottom:0}
.sidebarblock{border-style:solid;border-width:1px;border-color:#dbdbd6;margin-bottom:1.25em;padding:1.25em;background:#f3f3f2;-webkit-border-radius:4px;border-radius:4px}
.sidebarblock>:first-child{margin-top:0}
.sidebarblock>:last-child{margin-bottom:0}
.sidebarblock>.content>.title{color:#7a2518;margin-top:0;text-align:center}
.exampleblock>.content>:last-child>:last-child,.exampleblock>.content .olist>ol>li:last-child>:last-child,.exampleblock>.content .ulist>ul>li:last-child>:last-child,.exampleblock>.content .qlist>ol>li:last-child>:last-child,.sidebarblock>.content>:last-child>:last-child,.sidebarblock>.content .olist>ol>li:last-child>:last-child,.sidebarblock>.content .ulist>ul>li:last-child>:last-child,.sidebarblock>.content .qlist>ol>li:last-child>:last-child{margin-bottom:0}
.literalblock pre,.listingblock>.content>pre{-webkit-border-radius:4px;border-radius:4px;overflow-x:auto;padding:1em;font-size:.8125em}
@media screen and (min-width:768px){.literalblock pre,.listingblock>.content>pre{font-size:.90625em}}
@media screen and (min-width:1280px){.literalblock pre,.listingblock>.content>pre{font-size:1em}}
.literalblock pre,.listingblock>.content>pre:not(.highlight),.listingblock>.content>pre[class="highlight"],.listingblock>.content>pre[class^="highlight "]{background:#f7f7f8}
.literalblock.output pre{color:#f7f7f8;background:rgba(0,0,0,.9)}
.listingblock>.content{position:relative}
.listingblock code[data-lang]::before{display:none;content:attr(data-lang);position:absolute;font-size:.75em;top:.425rem;right:.5rem;line-height:1;text-transform:uppercase;color:inherit;opacity:.5}
.listingblock:hover code[data-lang]::before{display:block}
.listingblock.terminal pre .command::before{content:attr(data-prompt);padding-right:.5em;color:inherit;opacity:.5}
.listingblock.terminal pre .command:not([data-prompt])::before{content:"$"}
.listingblock pre.highlightjs{padding:0}
.listingblock pre.highlightjs>code{padding:1em;-webkit-border-radius:4px;border-radius:4px}
.listingblock pre.prettyprint{border-width:0}
.prettyprint{background:#f7f7f8}
pre.prettyprint .linenums{line-height:1.45;margin-left:2em}
pre.prettyprint li{background:none;list-style-type:inherit;padding-left:0}
pre.prettyprint li code[data-lang]::before{opacity:1}
pre.prettyprint li:not(:first-child) code[data-lang]::before{display:none}
table.linenotable{border-collapse:separate;border:0;margin-bottom:0;background:none}
table.linenotable td[class]{color:inherit;vertical-align:top;padding:0;line-height:inherit;white-space:normal}
table.linenotable td.code{padding-left:.75em}
table.linenotable td.linenos{border-right:1px solid currentColor;opacity:.35;padding-right:.5em}
pre.pygments .lineno{border-right:1px solid currentColor;opacity:.35;display:inline-block;margin-right:.75em}
pre.pygments .lineno::before{content:"";margin-right:-.125em}
.quoteblock{margin:0 1em 1.25em 1.5em;display:table}
.quoteblock:not(.excerpt)>.title{margin-left:-1.5em;margin-bottom:.75em}
.quoteblock blockquote,.quoteblock p{color:rgba(0,0,0,.85);font-size:1.15rem;line-height:1.75;word-spacing:.1em;letter-spacing:0;font-style:italic;text-align:justify}
.quoteblock blockquote{margin:0;padding:0;border:0}
.quoteblock blockquote::before{content:"\201c";float:left;font-size:2.75em;font-weight:bold;line-height:.6em;margin-left:-.6em;color:#7a2518;text-shadow:0 1px 2px rgba(0,0,0,.1)}
.quoteblock blockquote>.paragraph:last-child p{margin-bottom:0}
.quoteblock .attribution{margin-top:.75em;margin-right:.5ex;text-align:right}
.verseblock{margin:0 1em 1.25em}
.verseblock pre{font-family:"Open Sans","DejaVu Sans",sans;font-size:1.15rem;color:rgba(0,0,0,.85);font-weight:300;text-rendering:optimizeLegibility}
.verseblock pre strong{font-weight:400}
.verseblock .attribution{margin-top:1.25rem;margin-left:.5ex}
.quoteblock .attribution,.verseblock .attribution{font-size:.9375em;line-height:1.45;font-style:italic}
.quoteblock .attribution br,.verseblock .attribution br{display:none}
.quoteblock .attribution cite,.verseblock .attribution cite{display:block;letter-spacing:-.025em;color:rgba(0,0,0,.6)}
.quoteblock.abstract blockquote::before,.quoteblock.excerpt blockquote::before,.quoteblock .quoteblock blockquote::before{display:none}
.quoteblock.abstract blockquote,.quoteblock.abstract p,.quoteblock.excerpt blockquote,.quoteblock.excerpt p,.quoteblock .quoteblock blockquote,.quoteblock .quoteblock p{line-height:1.6;word-spacing:0}
.quoteblock.abstract{margin:0 1em 1.25em;display:block}
.quoteblock.abstract>.title{margin:0 0 .375em;font-size:1.15em;text-align:center}
.quoteblock.excerpt>blockquote,.quoteblock .quoteblock{padding:0 0 .25em 1em;border-left:.25em solid #dddddf}
.quoteblock.excerpt,.quoteblock .quoteblock{margin-left:0}
.quoteblock.excerpt blockquote,.quoteblock.excerpt p,.quoteblock .quoteblock blockquote,.quoteblock .quoteblock p{color:inherit;font-size:1.0625rem}
.quoteblock.excerpt .attribution,.quoteblock .quoteblock .attribution{color:inherit;font-size:.85rem;text-align:left;margin-right:0}
p.tableblock:last-child{margin-bottom:0}
td.tableblock>.content{margin-bottom:1.25em;word-wrap:anywhere}
td.tableblock>.content>:last-child{margin-bottom:-1.25em}
table.tableblock,th.tableblock,td.tableblock{border:0 solid #dedede}
table.grid-all>*>tr>*{border-width:1px}
table.grid-cols>*>tr>*{border-width:0 1px}
table.grid-rows>*>tr>*{border-width:1px 0}
table.frame-all{border-width:1px}
table.frame-ends{border-width:1px 0}
table.frame-sides{border-width:0 1px}
table.frame-none>colgroup+*>:first-child>*,table.frame-sides>colgroup+*>:first-child>*{border-top-width:0}
table.frame-none>:last-child>:last-child>*,table.frame-sides>:last-child>:last-child>*{border-bottom-width:0}
table.frame-none>*>tr>:first-child,table.frame-ends>*>tr>:first-child{border-left-width:0}
table.frame-none>*>tr>:last-child,table.frame-ends>*>tr>:last-child{border-right-width:0}
table.stripes-all tr,table.stripes-odd tr:nth-of-type(odd),table.stripes-even tr:nth-of-type(even),table.stripes-hover tr:hover{background:#f8f8f7}
th.halign-left,td.halign-left{text-align:left}
th.halign-right,td.halign-right{text-align:right}
th.halign-center,td.halign-center{text-align:center}
th.valign-top,td.valign-top{vertical-align:top}
th.valign-bottom,td.valign-bottom{vertical-align:bottom}
th.valign-middle,td.valign-middle{vertical-align:middle}
table thead th,table tfoot th{font-weight:bold}
tbody tr th{background:#f7f8f7}
tbody tr th,tbody tr th p,tfoot tr th,tfoot tr th p{color:rgba(0,0,0,.8);font-weight:bold}
p.tableblock>code:only-child{background:none;padding:0}
p.tableblock{font-size:1em}
ol{margin-left:1.75em}
ul li ol{margin-left:1.5em}
dl dd{margin-left:1.125em}
dl dd:last-child,dl dd:last-child>:last-child{margin-bottom:0}
ol>li p,ul>li p,ul dd,ol dd,.olist .olist,.ulist .ulist,.ulist .olist,.olist .ulist{margin-bottom:.625em}
ul.checklist,ul.none,ol.none,ul.no-bullet,ol.no-bullet,ol.unnumbered,ul.unstyled,ol.unstyled{list-style-type:none}
ul.no-bullet,ol.no-bullet,ol.unnumbered{margin-left:.625em}
ul.unstyled,ol.unstyled{margin-left:0}
ul.checklist{margin-left:.625em}
ul.checklist li>p:first-child>.fa-square-o:first-child,ul.checklist li>p:first-child>.fa-check-square-o:first-child{width:1.25em;font-size:.8em;position:relative;bottom:.125em}
ul.checklist li>p:first-child>input[type="checkbox"]:first-child{margin-right:.25em}
ul.inline{display:-ms-flexbox;display:-webkit-box;display:flex;-ms-flex-flow:row wrap;-webkit-flex-flow:row wrap;flex-flow:row wrap;list-style:none;margin:0 0 .625em -1.25em}
ul.inline>li{margin-left:1.25em}
.unstyled dl dt{font-weight:400;font-style:normal}
ol.arabic{list-style-type:decimal}
ol.decimal{list-style-type:decimal-leading-zero}
ol.loweralpha{list-style-type:lower-alpha}
ol.upperalpha{list-style-type:upper-alpha}
ol.lowerroman{list-style-type:lower-roman}
ol.upperroman{list-style-type:upper-roman}
ol.lowergreek{list-style-type:lower-greek}
.hdlist>table,.colist>table{border:0;background:none}
.hdlist>table>tbody>tr,.colist>table>tbody>tr{background:none}
td.hdlist1,td.hdlist2{vertical-align:top;padding:0 .625em}
td.hdlist1{font-weight:bold;padding-bottom:1.25em}
td.hdlist2{word-wrap:anywhere}
.literalblock+.colist,.listingblock+.colist{margin-top:-.5em}
.colist td:not([class]):first-child{padding:.4em .75em 0;line-height:1;vertical-align:top}
.colist td:not([class]):first-child img{max-width:none}
.colist td:not([class]):last-child{padding:.25em 0}
.thumb,.th{line-height:0;display:inline-block;border:solid 4px #fff;-webkit-box-shadow:0 0 0 1px #ddd;box-shadow:0 0 0 1px #ddd}
.imageblock.left{margin:.25em .625em 1.25em 0}
.imageblock.right{margin:.25em 0 1.25em .625em}
.imageblock>.title{margin-bottom:0}
.imageblock.thumb,.imageblock.th{border-width:6px}
.imageblock.thumb>.title,.imageblock.th>.title{padding:0 .125em}
.image.left,.image.right{margin-top:.25em;margin-bottom:.25em;display:inline-block;line-height:0}
.image.left{margin-right:.625em}
.image.right{margin-left:.625em}
a.image{text-decoration:none;display:inline-block}
a.image object{pointer-events:none}
sup.footnote,sup.footnoteref{font-size:.875em;position:static;vertical-align:super}
sup.footnote a,sup.footnoteref a{text-decoration:none}
sup.footnote a:active,sup.footnoteref a:active{text-decoration:underline}
#footnotes{padding-top:.75em;padding-bottom:.75em;margin-bottom:.625em}
#footnotes hr{width:20%;min-width:6.25em;margin:-.25em 0 .75em;border-width:1px 0 0}
#footnotes .footnote{padding:0 .375em 0 .225em;line-height:1.3334;font-size:.875em;margin-left:1.2em;margin-bottom:.2em}
#footnotes .footnote a:first-of-type{font-weight:bold;text-decoration:none;margin-left:-1.05em}
#footnotes .footnote:last-of-type{margin-bottom:0}
#content #footnotes{margin-top:-.625em;margin-bottom:0;padding:.75em 0}
.gist .file-data>table{border:0;background:#fff;width:100%;margin-bottom:0}
.gist .file-data>table td.line-data{width:99%}
div.unbreakable{page-break-inside:avoid}
.big{font-size:larger}
.small{font-size:smaller}
.underline{text-decoration:underline}
.overline{text-decoration:overline}
.line-through{text-decoration:line-through}
.aqua{color:#00bfbf}
.aqua-background{background:#00fafa}
.black{color:#000}
.black-background{background:#000}
.blue{color:#0000bf}
.blue-background{background:#0000fa}
.fuchsia{color:#bf00bf}
.fuchsia-background{background:#fa00fa}
.gray{color:#606060}
.gray-background{background:#7d7d7d}
.green{color:#006000}
.green-background{background:#007d00}
.lime{color:#00bf00}
.lime-background{background:#00fa00}
.maroon{color:#600000}
.maroon-background{background:#7d0000}
.navy{color:#000060}
.navy-background{background:#00007d}
.olive{color:#606000}
.olive-background{background:#7d7d00}
.purple{color:#600060}
.purple-background{background:#7d007d}
.red{color:#bf0000}
.red-background{background:#fa0000}
.silver{color:#909090}
.silver-background{background:#bcbcbc}
.teal{color:#006060}
.teal-background{background:#007d7d}
.white{color:#bfbfbf}
.white-background{background:#fafafa}
.yellow{color:#bfbf00}
.yellow-background{background:#fafa00}
span.icon>.fa{cursor:default}
a span.icon>.fa{cursor:inherit}
.admonitionblock td.icon [class^="fa icon-"]{font-size:2.5em;text-shadow:1px 1px 2px rgba(0,0,0,.5);cursor:default}
.admonitionblock td.icon .icon-note::before{content:"\f05a";color:#19407c}
.admonitionblock td.icon .icon-tip::before{content:"\f0eb";text-shadow:1px 1px 2px rgba(155,155,0,.8);color:#111}
.admonitionblock td.icon .icon-warning::before{content:"\f071";color:#bf6900}
.admonitionblock td.icon .icon-caution::before{content:"\f06d";color:#bf3400}
.admonitionblock td.icon .icon-important::before{content:"\f06a";color:#bf0000}
.conum[data-value]{display:inline-block;color:#fff!important;background:rgba(0,0,0,.8);-webkit-border-radius:50%;border-radius:50%;text-align:center;font-size:.75em;width:1.67em;height:1.67em;line-height:1.67em;font-family:"Open Sans","DejaVu Sans",sans-serif;font-style:normal;font-weight:bold}
.conum[data-value] *{color:#fff!important}
.conum[data-value]+b{display:none}
.conum[data-value]::after{content:attr(data-value)}
pre .conum[data-value]{position:relative;top:-.125em}
b.conum *{color:inherit!important}
.conum:not([data-value]):empty{display:none}
dt,th.tableblock,td.content,div.footnote{text-rendering:optimizeLegibility}
h1,h2,p,td.content,span.alt{letter-spacing:-.01em}
p strong,td.content strong,div.footnote strong{letter-spacing:-.005em}
p,blockquote,dt,td.content,span.alt{font-size:1.0625rem}
p{margin-bottom:1.25rem}
.sidebarblock p,.sidebarblock dt,.sidebarblock td.content,p.tableblock{font-size:1em}
.exampleblock>.content{background:#fffef7;border-color:#e0e0dc;-webkit-box-shadow:0 1px 4px #e0e0dc;box-shadow:0 1px 4px #e0e0dc}
.print-only{display:none!important}
@page{margin:1.25cm .75cm}
@media print{*{-webkit-box-shadow:none!important;box-shadow:none!important;text-shadow:none!important}
html{font-size:80%}
a{color:inherit!important;text-decoration:underline!important}
a.bare,a[href^="#"],a[href^="mailto:"]{text-decoration:none!important}
a[href^="http:"]:not(.bare)::after,a[href^="https:"]:not(.bare)::after{content:"(" attr(href) ")";display:inline-block;font-size:.875em;padding-left:.25em}
abbr[title]::after{content:" (" attr(title) ")"}
pre,blockquote,tr,img,object,svg{page-break-inside:avoid}
thead{display:table-header-group}
svg{max-width:100%}
p,blockquote,dt,td.content{font-size:1em;orphans:3;widows:3}
h2,h3,#toctitle,.sidebarblock>.content>.title{page-break-after:avoid}
#header,#content,#footnotes,#footer{max-width:none}
#toc,.sidebarblock,.exampleblock>.content{background:none!important}
#toc{border-bottom:1px solid #dddddf!important;padding-bottom:0!important}
body.book #header{text-align:center}
body.book #header>h1:first-child{border:0!important;margin:2.5em 0 1em}
body.book #header .details{border:0!important;display:block;padding:0!important}
body.book #header .details span:first-child{margin-left:0!important}
body.book #header .details br{display:block}
body.book #header .details br+span::before{content:none!important}
body.book #toc{border:0!important;text-align:left!important;padding:0!important;margin:0!important}
body.book #toc,body.book #preamble,body.book h1.sect0,body.book .sect1>h2{page-break-before:always}
.listingblock code[data-lang]::before{display:block}
#footer{padding:0 .9375em}
.hide-on-print{display:none!important}
.print-only{display:block!important}
.hide-for-print{display:none!important}
.show-for-print{display:inherit!important}}
@media print,amzn-kf8{#header>h1:first-child{margin-top:1.25rem}
.sect1{padding:0!important}
.sect1+.sect1{border:0}
#footer{background:none}
#footer-text{color:rgba(0,0,0,.6);font-size:.9em}}
@media amzn-kf8{#header,#content,#footnotes,#footer{padding:0}}
</style>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
<link rel="stylesheet" href="C:/Users/Ferd/git/klab/docs/etc/highlightjs4klab/build//styles/klab.min.css">
</head>
<body class="article toc2 toc-left">
<div id="header">
<h1>k.LAB: a semantic web platform for science</h1>
<div class="details">
<span id="author" class="author">Ferdinando Villa</span><br>
<span id="email" class="email"><a href="mailto:ferdinando.villa@bc3research.org">ferdinando.villa@bc3research.org</a></span><br>
<span id="revdate">Technical note 2021-02-20</span>
</div>
<div id="toc" class="toc2">
<div id="toctitle">Table of Contents</div>
<ul class="sectlevel1">
<li><a href="#_architecture_of_the_k_lab_platform">1. Architecture of the k.LAB platform</a>
<ul class="sectlevel2">
<li><a href="#_the_software_stack">1.1. The software stack</a></li>
<li><a href="#_the_k_lab_logical_layers">1.2. The k.LAB logical layers</a></li>
<li><a href="#_accessing_the_system">1.3. Accessing the system</a></li>
</ul>
</li>
<li><a href="#_the_resource_layer">2. The resource layer</a>
<ul class="sectlevel2">
<li><a href="#_lifecycle_of_k_lab_resources">2.1. Lifecycle of k.LAB resources</a></li>
</ul>
</li>
<li><a href="#_the_semantic_layer_semantic_modeling">3. The semantic layer: semantic modeling</a>
<ul class="sectlevel2">
<li><a href="#_semantic_mediation_and_inference_in_support_of_modeling">3.1. Semantic mediation and inference in support of modeling</a></li>
<li><a href="#_the_worldview">3.2. The worldview</a>
<ul class="sectlevel3">
<li><a href="#_authorities">3.2.1. Authorities</a></li>
</ul>
</li>
<li><a href="#_learning_models">3.3. Learning models</a></li>
<li><a href="#_sessions_and_outputs_of_contextualization">3.4. Sessions and outputs of contextualization</a></li>
<li><a href="#_extending_the_runtime_system">3.5. Extending the runtime system</a></li>
<li><a href="#_integrating_external_models">3.6. Integrating external models</a>
<ul class="sectlevel3">
<li><a href="#_integrating_k_lab_into_existing_models">3.6.1. Integrating k.LAB into existing models</a></li>
<li><a href="#_integrating_existing_models_into_k_lab">3.6.2. Integrating existing models into k.LAB</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#_the_reactivity_layer_behaviors_and_applications">4. The reactivity layer: behaviors and applications</a>
<ul class="sectlevel2">
<li><a href="#_user_side_applications">4.1. User-side applications</a></li>
</ul>
</li>
<li><a href="#_current_status">5. Current status</a></li>
</ul>
</div>
</div>
<div id="content">
<div id="preamble">
<div class="sectionbody">
<div style="page-break-after: always;"></div>
<div class="paragraph">
<p><strong>Integrated modeling</strong> is a practice meant to maximize the value of scientific information by ensuring its  <em>modularity</em>, <em>reusability</em>, <em>interoperability</em> and <em>traceability</em> throughout the scientific process. The k.LAB software, discussed here, is a full-stack solution for integrated modelling, supporting the production, curation, linking and deployment of scientific artifacts such as datasets, data services, modular model components and distributed computational services. The purpose of k.LAB is to ensure&#8201;&#8212;&#8201;by <em>design</em> rather than intention&#8201;&#8212;&#8201;that the pool of such artifacts constitutes a seamless <em>knowledge commons</em>, readily actionable (by humans or machines) through a full realization of the <em>linked data</em> paradigm augmented with semantics and powered by artificial intelligence. This design enables automation of a wide range of modeling tasks that were previously only performable by experts and on an ad-hoc basis.</p>
</div>
<div class="paragraph">
<p>The k.LAB platform directly addresses the four FAIR goals (Findable, Accessible, Interoperable and Reusable), introducing innovation particularly in the practice of <strong>semantic annotation</strong>, which is reinvented as a modern, expressive approach meant to ease adoption by both providers and users. To the four dimensions in FAIR, k.LAB adds a <em>reactivity</em> dimension, in line with the original vision of a semantic web: this dimension enables knowledge to also be <em>deployed</em> in an <em>"internet of observations"</em>, creating <em>live</em> artifacts that can interact, improve and evolve as new information appears on the network.</p>
</div>
<div class="paragraph">
<p>The central service in the k.LAB modeling API wraps the <em>resolution algorithm</em>, which receives as input a logical query of the form "observe <em>concept</em> in <em>context</em>" (e.g., "observe <em>change in land cover type</em> in <em>Colombia, 2015-2020</em>", only slightly paraphrased from k.LAB&#8217;s near-natural language query formalism) and, in response, assembles, documents, initializes and runs a computation (called a <em>dataflow</em>) that produces the  <strong>observation</strong> of the concept that best fits the context, based on the integration of data and model components available in the distributed k.LAB network. The observations output by the API request, along with the dataflow assembled to generate them, are themselves scientific artifacts&#8201;&#8212;&#8201;automatically augmented with provenance records and user-readable documentation&#8201;&#8212;&#8201;that can be exported and curated as needed.</p>
</div>
<div class="paragraph">
<p>Artificial intelligence, driven by both semantics (<em>machine reasoning</em>) and the analysis of previous outcomes (<em>machine learning</em>), satisfies the request using a shared, communally owned and curated knowledge base (the <em>worldview</em>, a set of ontologies) and the resource pool available at any given moment on the k.LAB network, by ranking, selecting, adapting, and connecting data and model components made available by independent and uncoordinated providers. We refer to the process of building the computational strategy that "observes" a concept in a context as <em>resolution</em>, and to its execution to produce the desired result as <em>contextualization</em>, although we also refer to the combination of both processes when we discuss the "resolution service" provided by the API.</p>
</div>
<div class="paragraph">
<p>This document describes the main principles and architecture of k.LAB. More detailed reference documentation for k.LAB is in development and will be referenced in this document where available.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_architecture_of_the_k_lab_platform">1. Architecture of the k.LAB platform</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The open source k.LAB software stack includes client and server components that support the creation, maintenance and use of a distributed <em>semantic web platform</em> where scientific information can be stored, published, curated and connected. The software is licensed through the Affero General Public License (AGPL) v.3.0; the core components are available as a single project in the <a href="https://bitbucket.org/integratedmodelling/klab">k.LAB git repository</a>.</p>
</div>
<div class="sect2">
<h3 id="_the_software_stack">1.1. The software stack</h3>
<div class="ulist">
<ul>
<li>
<p><strong>Server</strong> components are deployed by certified <em>partners</em> to publish resources and semantic content (<strong>k.LAB Node</strong>) and/or to provide modeling services and applications (<strong>k.LAB Engine</strong>) to online users. Published resources can include both static data and dynamic computations, both of which may be hosted in source form at the node or linked to external data (e.g. WCS, WFS, OpenDAP) or computational services (e.g. OpenCPU). The k.Node software is deployed in containers that can be configured to host dedicated instances of Geoserver, PostgreSQL, Hyrax or other services; these are transparently managed through server adapters inside the node, virtually eliminating the need for specific training on those components for node administrators.</p>
</li>
<li>
<p><strong>Client</strong> components are used by contributors to develop, validate and publish resources and semantic content (<strong>k.Modeler</strong>, an Integrated Development Environment (IDE) for semantic modeling), and by end users (<strong>k.Explorer</strong>, a web-based application environment) to access modeling services and specialized applications built for the platform and delivered through the web.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Additional server components serve specific needs on the k.LAB network and are of less common application in partner sites. Among them the following are noteworthy:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The <em>hub</em> server, <strong>k.Hub</strong>, manages authentication and organizes node access for authenticated engines. The Integrated Modeling Partnership manages a set of nodes and a main hub, and releases site certificates that enable nodes to be connected to form the platform. Partners that need to manage users locally may also deploy and connect a hub, although this is normally only convenient in large deployments.</p>
</li>
<li>
<p>A <em>semantic server</em> collects and indexes the semantic knowledge from the worldview and all public projects, constantly compiling and revising documentation and use cases to assist users in the semantic annotation process. Users can look up annotations made by others and access hyperlinked, evolving descriptions of each concept and predicate. The semantic server can be connected to the k.Modeler editor to provide inline logical validation of logical expressions in models being developed, and a suggestion service that can find and propose comparisons with use cases extracted from peer-reviewed public projects. Through the use of specialized metadata inserted in k.IM source files, the server can be integrated with the editors so that assistance is available directly, to ease the development of semantic content as much as possible. The semantic server is in development and is not available to the general public yet.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Other, less critical server components are in development and are not discussed here. Among these, a statistics server collects anonymized information from successful and unsuccessful resolutions and processes them using machine learning techniques to improve the resolution algorithm.</p>
</div>
</div>
<div class="sect2">
<h3 id="_the_k_lab_logical_layers">1.2. The k.LAB logical layers</h3>
<div class="paragraph">
<p>The set of active, connected nodes and engines at any given time forms what can be seen collectively as a distributed container, where scientific knowledge is found in <strong>three layers</strong> handling information at increasing levels of abstraction: the <em>resources</em>, <em>semantic</em> and <em>reactivity</em> layers. The first can be seen as a data curation platform based on modern linked data concepts, using a generalized and flexible data model. Semantic and reactive content for the platform is developed in the respective layers using two specialized languages, <em>k.IM</em> for semantic resources and <em>k.Actors</em> for reactive behaviors and applications. The modeler IDE (<em>k.Modeler</em>) provides drag-and-drop interfaces to build resources and a specialized editing and debugging environment for k.LAB projects, supporting both k.IM and k.Actors development.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The <strong>resource</strong> layer provides a <em>protocol</em> for conventional data and computational resources or services to interoperate at the data level, matching identifiers, data types and metadata through a uniform API. Nodes and client applications include interfaces to manage development and submission of knowledge to the resources layer, to be published and curated either on-site or through hosting providers with full control of licensing and access.</p>
</li>
<li>
<p>The <strong>semantic</strong> layer provides a <em>language</em> that enables annotating resources through semantically explicit logical expressions, ensuring findability, interoperability and accessibility through purely logical queries, validating consistency and producing mediation strategies through machine reasoning and logical inference. The semantic layer uses the <strong>k.IM</strong> language to specify semantic knowledge (compatible with W3-endorsed <a href="https://www.w3.org/TR/owl-guide/">OWL 2</a>) and models; these specifications, collected into namespaces and projects, can be deployed to k.LAB Nodes for the k.LAB inference engines to find, rank and use.</p>
</li>
<li>
<p>The <strong>reactivity</strong> layer provides <em>behaviors</em> for the scientific artifacts produced by running queries in the semantic layer, effectively turning observations into software agents. Such reactive observations  can generate and react to events either locally (within the same session) or remotely. The reactivity layer enables distributed agent-based simulations and computations that automatically adapt to changing conditions or states. The <strong>k.Actors</strong> language is used to define behaviors for the reactivity layer. As a special case, behaviors bound to users and sessions can be used to quickly develop specialized interactive applications that run on the k.LAB Explorer platform, accessed through web browsers.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The separation of concerns and APIs in the three layers maximizes their value: for example, the resources layer can be seen through different semantics, therefore serving different purposes in different networks by reinterpreting it through the logical "lens" of a differently configured semantic layer.</p>
</div>
</div>
<div class="sect2">
<h3 id="_accessing_the_system">1.3. Accessing the system</h3>
<div class="paragraph">
<p>The k.LAB system can be accessed through client software (usually an application running in a browser within the k.LAB Explorer web platform, or the k.LAB integrated development environment (IDE), k.Modeler) or through its API by software applications. Providers of content may use the IDE or, in the near future, a content provider web interface available on all k.LAB nodes, including of course any nodes deployed at the provider&#8217;s end. All users must be authenticated through a valid, secure certificate, which also establishes the semantic <em>worldview</em> of reference and any user permissions for the certificate holder. Permissions may limit access to reserved or private content, which may be made available in public form or be linked to specific users or groups thereof by its owners.</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">Regular users</dt>
<dd>
<p>Non-technical users of the k.LAB platform normally interact with the system through an instance of k.LAB Explorer exposed by a networked k.LAB engine (or cluster thereof). The basic k.LAB Explorer interface is usable as a generic search-and-compute interface and allows users to easily set their context of interest to locations and times of interest. Queries are cached and suggestions are given based on the user&#8217;s groups and previous queries, providing an experience similar to modern search platforms. As k.LAB Explorer can be used as an application development platform (see further in this document), specific applications can be built on top of k.LAB Explorer and given a specialized access URL. Such applications, like the recently deployed <a href="https://seea.un.org/content/aries-for-seea">ARIES for SEEA</a>, look and feel like typical interactive web applications and can be developed and deployed with very minimal effort to assist specific classes of users.</p>
</dd>
<dt class="hdlist1">Content providers and modelers</dt>
<dd>
<p>The k.LAB engine, a server-side component, can also be run at the client side in a local configuration, so that new content can be developed and tested in a sandboxed environment before publishing, with full access to public resources. Such client use is supported and facilitated by a small, downloadable <a href="https://integratedmodelling.org/get_started">control center application</a> that removes the complexities linked to installing, upgrading, starting and stopping the engine or the k.Modeler IDE. The IDE remains, at the time of this writing, the endorsed toolkit to prepare both semantic and non-semantic content for distribution and publish it to the network. In the near future, more direct pathways will be enabled so that data contributors can also provide content (particularly datasets) through less technical, web-based interfaces.</p>
</dd>
<dt class="hdlist1">Applications and software</dt>
<dd>
<p>The k.LAB system provides a stable API for all its server components, more notably the authenticating hub, the nodes and the engines. This API is used by all the k.LAB client software but can be used independently to enact a "modeling as a service" paradigm whose primary service provided is the resolution algorithm. At the time of this writing, the API is mostly used through k.LAB&#8217;s own client software, but ongoing projects and collaborations point to a more widespread integration of k.LAB API services within foreign platforms and applications in the next months. In addition to direct use of REST endpoints, served by engine clusters operated by BC3 and partner institutions, client libraries for popular languages (Python, Javascript, R) will be made available based on demand to ease integration with existing applications.</p>
</dd>
</dl>
</div>
<div class="paragraph">
<p>In addition to uploading content to existing nodes, institutionals contributors can deploy k.LAB Node software to deploy sites that contribute to the k.LAB network while remaining fully in control of all distribution details. Nodes are deployed as containers that can be easily set up and authorized by certified partners. The k.LAB distributed paradigm supports and enforces a model where information remains under the ownership of its authoritative sources while maximizing its availability and interoperability, compatibly with both public and commercial services, thanks to careful attribution of ownership and to state-of-the-art encryption, access control and security.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_the_resource_layer">2. The resource layer</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The resource layer contains or provides access to all "conventional", non-semantic informational assets available to k.LAB: from raw datasets and bridges to external data services to algorithms expressed as mathematical equations or computable code.</p>
</div>
<div class="paragraph">
<p>The aim of the resource layer is to present common conventions and a consistent API for k.LAB to access and manage pre-existing data, models and services of all kinds, as a first layer of interoperability. While the semantic layer specifies a <em>language</em> for interoperability, the resource layer provides a generic <em>protocol</em> that can be adapted to any existing data source or service as well as databases and external computations. Because resources have no semantics associated, it is possible to <em>reinterpret</em> any resources through the desired semantics, enabling complete orthogonality between the resource layer and the semantic layer.</p>
</div>
<div class="paragraph">
<p>Importantly, in k.LAB, <em>computations</em>, from simple equations to large and complex models, can also live in the resource layer. In fact, anything that takes inputs and produces outputs in numeric or other form, with no meaning explicitly attached beyond names and metadata, can be seen as a k.LAB resource. All resources are identified by a Uniform Resource Name (URN) which can be resolved, through the k.LAB API, to an informational record that contains all original metadata along with provenance information, history, and access permissions for the requesting user. Inputs, outputs and (in the case of resources that produce multiple objects) attributes will be similarly identified by a name and a data type.</p>
</div>
<div class="paragraph">
<p>Resources are not used directly by external clients in normal k.LAB usage, although the resource API is open to authorized users and can be used as the base layer of a standard <a href="https://www.w3.org/TR/ldp/"><em>linked data</em></a> platform. The resource URN is, instead, used in semantic models (see the next section) that in turn populate the search space for the semantically-driven resolution algorithm at the core of the system. A model that references a URN which the requesting user has no access to is automatically deactivated and does not participate in resolution, allowing the k.LAB resolver to continue resolving through another authorized strategy. Where semantic assets have <em>semantics</em> and <em>scale</em>, all resources have <em>data types</em> (number, text, boolean, or probability distributions thereof) and a <em>geometry</em>, which defines the original representation of space and time, if any, in the resource.</p>
</div>
<div class="paragraph">
<p>A resource URN is a fully specified identifier that uniquely references a resource in k.IM namespaces. It consists of 4 parts, separated by colons:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>A <strong>node name</strong> (the name of the node where the resource was originally published);</p>
</li>
<li>
<p>A <strong>catalog</strong> (a logical space handled by the node, for example a domain such as hydrology, or a name describing a large-scale collection of data);</p>
</li>
<li>
<p>A <strong>namespace</strong> (a secondary logical space within the catalog);</p>
</li>
<li>
<p>A <strong>resource identifier</strong>.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The node name may consist of the reserved word <code>local</code> (identifying an unpublished local resource, see below) or <code>klab</code> to identify a "universal" resource with no network storage associated and handled directly by the engine, where the catalog part of the URN denotes a specific software adapter (e.g. <code>klab:random:</code> would introduce a URN pattern that produces various types of random data for testing, defined by the namespace and identifier). The core resource API, exposed by both k.LAB Engine and k.LAB Node, provides a URN resolution service (URN &#8594; resource metadata), standard Create/Read/Update/Delete (CRUD) operations on the resource layer, and the most important operation, <em>contextualization</em>, which takes as input a resource URN and a geometry specification and returns the data content of the resource adapted to the passed geometry. The contextualization return value is a flexible data structure (based on <a href="https://developers.google.com/protocol-buffers">Google Protobuf</a>) that allows efficient marshalling of zero or more objects, each with an internal structure that admits scalar or distributed values along grids or tessellations, conformant to the request geometry. The result, complete with metadata and provenance information but still devoid of semantics, is passed to the k.LAB runtime to be turned into observations within the execution of a k.LAB <em>dataflow</em>.</p>
</div>
<div class="paragraph">
<p>Not all resources occupy physical storage on k.LAB nodes: in fact, k.LAB extends the notion of the URN to encompass literals (e.g. <code>model 100 as geography:Elevation in m</code>, where <code>100</code> can be seen as a shorthand form of <code>klab:literals:values.parsed:number#value=100</code>) and specialized computational services which may simply serve as bridge to online services or computations (e.g. the urn <code>klab:osm:relations:park</code> would contextualize to all the parks stored as relations (polygons) in the <a href="https://www.openstreetmap.org">OpenStreetMap</a> service in the queried geometry).</p>
</div>
<div class="paragraph">
<p>Diverse, extendible sourcing of information for resources is enabled through the use of <em>adapters</em>, i.e. software plug-ins that adapt a specific data or service format to the API. The adapter identifier and all adapter parameters are specified in the metadata associated to the URN and used to select the methods for contextualization, import, export and indexing. Adapters are made available as k.LAB <em>components</em>, installable in k.LAB Engines and k.LAB Nodes, and can be extended by developers using the Java API to support formats and services not yet available. External APIs (e.g. datacubes) can be supported by deploying a bridge adapter, as long as the original service provides all the information needed for k.LAB to operate. To date, adapters for many file formats (CSV files, spatial rasters and vectors, NetCDF), protocols (WCS, WFS, OpenDAP, SDMX) and specialized services (OpenStreetMap, weather station data bridging to multiple databases and sources) are available and others (such as RDF/SPARQL) are in development. Other adapters enable specialized services, like scale-dependent selection of hierarchially organized datasets such as administrative regions or river basins. URN parameters can be added to the base URN to trigger specialized processing at the node&#8217;s end, such as resolution-dependent simplification of polygons, selection of interpolation methods, or any other adapter-dependent option that will best suit the desired semantics.</p>
</div>
<div class="sect2">
<h3 id="_lifecycle_of_k_lab_resources">2.1. Lifecycle of k.LAB resources</h3>
<div class="paragraph">
<p>Resources start their life as <em>local</em> within a user project, imported from files or through a resource editor integrated with the k.Modeler client software. Such local resources go through a process of validation, meant to ensure that every need of the k.LAB system can be satisfied in an integrated scenario of use: for example, spatial data must have proper projections and valid polygons throughout. When a local resource is accepted, it can be used inside the project that contains it or in any other project that shares the same local workspace, but is not visible to other users of the platform. Local resources may be sufficient for the needs of a specific, short-term project; yet, the natural lifecycle of a resource continues with <em>publication</em>, which makes it available across the k.LAB network.</p>
</div>
<div class="paragraph">
<p>Publication of a resource is conditional to further validation; no resources with incomplete metadata, licensing or ownership information is accepted by the software. Successful publication uploads the resource to the staging area of a chosen k.LAB node, where it can be made available for general use and further edited in-place by its owner. Every edit of a published resource creates a new version of the resource and full history is kept. Published resources are independent of projects and obtain a unique URNs that never changes; the hosting k.LAB Node may optimize their data content for faster serving and automatically mirror the resource to other nodes for increased availability. While public resources may be visibile, at the choice of their owner, only to selected users or groups of users, their URLs are universally recognized and can be used in k.IM models without the need for any registration or download, as long as the user is connected to the k.LAB network.</p>
</div>
<div class="paragraph">
<p>The staging "tier" of the resource layer is, at the time of this writing, the only one enabled in k.LAB Node software. It is envisioned that an iterative resource review process, operated by willing members of the community, will be used to promote resources to higher-ranking tiers, the level of which may affect the resolution algorithm, also incorporating user feedback and machine-learned statistics as resources get used as part of models. This process may eventually involve the attribution of a DOI to resources, resolved both through standard DOI proxy servers and directly by k.LAB, enabling use of the DOI in place of the URN in semantic models.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_the_semantic_layer_semantic_modeling">3. The semantic layer: semantic modeling</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Semantic modeling enables the <em>semantic annotation</em> of non-semantic resources based on a shared <em>worldview</em> (a logically organized knowledge base containing concepts and relationships). The linking of semantics to resource URNs is done in <em>models</em>, i.e. semantic annotations that specify the meaning associated with resources and, when applicable, with their inputs, outputs and attributes. As a non-semantic resource can represent both data and computations, k.LAB treats data annotations and semantically annotated algorithms uniformly; as a result, we use the term <em>model</em> to refer to both. The pool of models connected to k.LAB sessions, organized into  <em>projects</em> made available on k.LAB nodes, constitutes the semantic layer, which is searched by the resolution algorithm to resolve a logical query to a result artifact.</p>
</div>
<div class="paragraph">
<p>All semantic assets, from the knowledge base itself (concepts, relationships) to all semantically annotated content (data, algorithms) are specified in the <strong>k.IM language</strong>. While the underlying knowledge model for k.IM is the W3 standard OWL2 (to which all logical k.IM specifications can be translated), k.IM&#8217;s close resemblance to the structure of the English language makes it uncommonly readable:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-kim hljs" data-lang="kim">model occurrence of agriculture:Pollinator biology:Insect caused by earth:Weather
	observing
		earth:AtmosphericTemperature in Celsius named air_temperature,
		earth:SolarRadiation in J named solar_radiation
	set to [0.62 + 1.027 * air_temperature + 0.006 * solar_radiation];</code></pre>
</div>
</div>
<div class="paragraph">
<p>In a departure from other ontology platforms, k.LAB admits, for the specification of semantics, logical expressions that combine predicates, operators and nouns in a fashion modeled on the grammar of the English language. For example, the k.IM statement <code>im:Net value of ecology:Pollination</code> (an <em>observable expression</em>, or <em>observable</em> in short) contains a predicate (<code>im:Net</code>) and a semantic operator <code>value of</code> which affects the meaning of the process concept <code>ecology:Pollination</code> and transforms it into the concept representing its quantifiable value. This <em>linguistic</em> articulation is key not only to the usability and parsimony of the underlying knowledge base, which can remain small and learnable thanks to the ability to combine and reuse terms and operators, but also to the functioning of the machine reasoning underlying the resolution algorithm, which can reason independently on the different logical dimensions of an observable and infer computations that would otherwise require specialized, <em>ad-hoc</em> modeling. Lacking specific models for a complex observable, each logical dimension of it may be resolved to one or more models which handle that specific component, and the set of models, ranked for best fit to the context before selecting the most appropriate, can be used to assemble the best-case computation to produce the finished observation. The resulting <em>dataflow</em> (algorithm) can, if wished, be saved as a non-semantic resource for future reference and reproducible reuse in k.LAB.</p>
</div>
<div class="paragraph">
<p>The specialized k.IM editor provided with k.Modeler further facilitates the use and recognition of semantics by color-coding the fundamental classes of knowledge represented by concepts (blue for predicates, such as attributes, roles, realms or identities; green for quantifiable or categorizable qualities; red, green/yellow, yellow and brown respectively for processes, events, relationships and subjects) <sup class="footnote">[<a id="_footnoteref_1" class="footnote" href="#_footnotedef_1" title="View footnote.">1</a>]</sup>. The editor is connected to the inference engine and further assists the modeler by checking the logical consistency of each observable as the user types and reporting any inconsistency as a syntax error. It is typical of k.LAB models to be very short, simple and easily readable. Every model, with few exceptions, resolves <em>one</em> observable expression, with any required inputs stated merely as semantics; as a result, each model, by design, can be run and tested independently. For example, the model below</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-kim hljs" data-lang="kim">model occurrence of earth:Region with im:Still earth:PrecipitationVolume
	observing
		earth:Upstream im:Area in m^2 named contributing_area,
		geography:Slope in degree_angle named slope
	set to [
		def sloperadians = Math.tan((slope*1.570796) / 90)
	  	return Math.log((contributing_area+1) / Math.tan((sloperadians+0.001)));
	];</code></pre>
</div>
</div>
<div class="paragraph">
<p>requires observations of geographical slope and upstream drainage area to compute its output, a commonly used hydrological quantity (topographic wetness index) interpreted here through the semantics of "occurrence of region with retained precipitation". None of the complex calculations required to compute the inputs needs to be part of the model, as their semantics (<code>earth:Upstream im:Area</code> and <code>geography:Slope</code>) is resolved at run time to the most appropriate model for the context when the primary observable is queried. The context can consist of a single point in space or of a gridded or polygon-based spatial coverage, without any modification to the model. If the context is temporally dynamic and the underlying state of a dependency (e.g. the slope) changes in time, the k.LAB runtime will automatically notice the change and recompute the output, unless a specific model of <code>change in occurrence of earth:Region with im:Still earth:PrecipitationVolume</code> (a process affecting the quality after the <code>change in</code> operator) can be resolved in the context. When the model logics require that certain dependencies are satisfied in a specific way, scoping rules in k.IM can be used to ensure that specific models (or models for a specified subset) are chosen to satisfy the desired dependencies. It is also possible to use (libraries of) <em>non-semantic models</em> to refer to specific computations whose semantics is deemed not worth exposing, ensuring linkage with conventionally used metrics without sacrificing modularity or requiring overly difficult semantic characterization.</p>
</div>
<div class="paragraph">
<p>In many situations, models can be written independent of the specific spatial and temporal context in which they will be run, and often even in ways that are compatible with different interpretations of space and time. When that is not convenient, language constructs can be used to lock a model or namespace so that it is only applied to specified representations or ranges of extents and/or resolutions in both space and time, as well as to override the priorities in the resolution algorithm to handle any special need of the models or of the resources they use. Negotiation of inputs, outputs, data format, units or currencies, visualization and contextual validation are by default left to the k.LAB runtime. Writing models this way enforces discipline and maximizes clarity, readability and parsimony: contributors only write the core of the algorithm that leads to one specific observation, leaving every other aspect (including the selection and computation of any inputs) to the resolver and the k.LAB runtime.</p>
</div>
<div class="sect2">
<h3 id="_semantic_mediation_and_inference_in_support_of_modeling">3.1. Semantic mediation and inference in support of modeling</h3>
<div class="paragraph">
<p>In simple cases, the query "observe <em>observable</em> in <em>context</em>" is answered by locating a model annotating a data source as an observation of the specified observable. For example, setting the context to a geographical region (e.g. a country&#8217;s extent with a spatial grid model at 100m resolution and temporal context, e.g. the year 2010) and querying an observable such as <code>geography:Elevation in m</code> may retrieve, among others, the following model:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-kim hljs" data-lang="kim">model im.data:geography:morphology:dem90 as geography:Elevation in m;</code></pre>
</div>
</div>
<div class="paragraph">
<p>which annotates a network-available resource specified by the URN <code>im.data:geography:morphology:dem90</code> as an observation of the <code>geography:Elevation</code> concept. The URN gives access to metadata including the original spatial and temporal coverage and resolution, through which the model, whose semantics is identical to the query&#8217;s, can be ranked for match to the context. If the model is deemed to be the best match, the k.LAB engine will translate it into a set of processing steps (in this case simply a resource retrieval operation plus any necessary mediation) and pass the resulting <em>dataflow</em> to the runtime to compute and produce the resulting <em>observation</em>, in this case a raster map of elevation, with 100m resolution, reflecting the boundaries and time of the context. The dataflow will include any necessary reprojection, resampling, or unit transformation to match the query and the context. Other models may compete for the choice, made on the basis of criteria such as resolution and extent match, specificity, semantic match, and including criteria such as peer review results or usage feedback for the original data. If the chosen model only partially covers the context, additional models may contribute to its complete characterization, as long as their ranks are close enough.</p>
</div>
<div class="paragraph">
<p>Besides such simple and direct matches, machine reasoning backed by an observation-centered (as opposed to reality-centered) ontological framework can enable more sophisticated observation tasks that do not correspond to readily available annotations and are normally only possible through specialized, time-consuming work. In a straightforward example, attributes such as <code>im:Normalized</code> may be prepended to another observable to affect the result, where the attribute would be resolved to an independent model (<code>model im:Normalized using &lt;normalization function&gt;</code>), possibly restricted to certain classes of observables (e.g. <code>model im:Normalized of im:Quantity &#8230;&#8203;</code> to restrict its application to numerically quantifiable observables) and used to modify a straight observation of <code>geography:Elevation</code> if the normalized observable cannot be resolved directly. More interestingly, resolution strategies may cross inherency barriers to infer the best observation strategy when a direct match is not available. For example, a hypothetical query for <code>(ecology:AboveGround ecology:Biomass) of biology:Eucalyptus biology:Tree</code> <sup class="footnote">[<a id="_footnoteref_2" class="footnote" href="#_footnotedef_2" title="View footnote.">2</a>]</sup> operated in the same country context would refer, by virtue of the inherency operator <code>of</code>, to a quality (above-ground biomass) inherent to a particular subset (Eucalyptus) of the observations of a secondary subject (Tree) located in the primary context of the query (a geographical region). It would be resolved by the following strategy:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Locate a model for the original observable, <code>(ecology:AboveGround ecology:Biomass) of biology:Eucalyptus biology:Tree</code>, that is compatible with the context of observation. If found, resolve using it. Otherwise</p>
</li>
<li>
<p>Locate a model of the inherent subject, <code>biology:Eucalyptus biology:Tree</code>; if found, accept it as the strategy to instantiate an observation for each eucalyptus tree in the region, so that a model of <code>(ecology:AboveGround ecology:Biomass)</code> can be later resolved in the context of each tree. If an "eucalyptus tree" model cannot be resolved</p>
</li>
<li>
<p>Locate a model capable of instantiating every <code>biology:Tree</code> in the region; if found, locate a classifier model capable of either 1) checking if the tree is eucalyptus or not (<code>model biology:Eucalyptus of biology:Tree</code>), or 2) attributing the abstract identity (<code>biology:Species</code>) of which <code>biology:Eucalyptus</code> is a subclass (<code>model biology:Species of biology:Tree</code>). Such a model would be applied to classify the tree observations, only keeping those that classify as eucalyptus.</p>
</li>
<li>
<p>If eucalyptus trees are resolved successfully through either strategy (2) or (3), locate a model of <code>(ecology:AboveGround ecology:Biomass)</code> for each tree to compute the biomass in the context of each. If successful, insert a <em>dereifying</em> operation to complete the observation, turning the "above ground biomass" values observed in the context of each tree into the quality "above ground biomass of eucalyptus tree" observed in the context region.</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>Similar reasoning strategies can be applied to a large set of situations, using semantic inference driven by the phenomenological understanding of the entities involved and the observation process applied to them. For example, a query for <code class="source kim">presence of biology:Tree</code> could be satisfied, when not resolvable directly, by a model of <code>(ecology:AboveGround ecology:Biomass) of biology:Tree</code> because biomass (a <code>im:Mass</code> in a higher-level ontology) is an <em>extensive</em> property, therefore its non-zero value implies the existence of its inherent subject. The presence can be computed as a true/false value attributed to the context wherever the biomass of any tree is nonzero. In another commonly encountered use case, qualities that can only be correctly computed in specifically delineated contexts (for example hydrological qualities, such as "upstream area", which only produce correct results when computed in a correctly delineated river basin) can be automatically computed in arbitrary contexts by first looking up a model to delineate all the relevant contexts (river basins) intersecting the areas, then applying the necessary models to compute the qualities inherently to those, then re-distributing the values over the desired context. Such behavior can be automated simply on semantic grounds by defining a concept such as</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-kim hljs" data-lang="kim">area ContributingArea
	is earth:Upstream im:Area within hydrology:RiverBasin;</code></pre>
</div>
</div>
<div class="paragraph">
<p>or, more correctly, leaving the observable unconstrained and defining models as</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-kim hljs" data-lang="kim">model earth:Upstream im:Area within hydrology:RiverBasin
	....;</code></pre>
</div>
</div>
<div class="paragraph">
<p>In both cases the 'within' operator mandates a RiverBasin context for the quality, which will trigger the distributed resolution process described whenever ContributingArea is queried in any context where river basins can be observed. The same considerations hold for more complex observables such as processes, which have the ability to affect the value of qualities through time and to generate events or other objects; these, in turn, can be the context for other qualities or processes. The ability to automatically negotiate mediations based on inherency and phenomenological reasoning multiplies the capability of connecting diverse models without error, offering integration possibilities orders of magnitude beyond those allowed by semantic matching alone. Such tasks require specific planning and significant technical expertise and time to perform in conventional ways.</p>
</div>
<div class="paragraph">
<p>Much of the power of k.LAB comes from the fact that models pertaining to the different sides of a problem may be provided and shared by independent experts, with no need for any coordination besides adhering to the same worldview. Each model can serve multiple potential purposes and does not just <em>add to</em>, but rather <em>multiplies</em> the value of other knowledge on the platform when interacting with it, just like words in natural language. The power of the resulting paradigm shift becomes obvious when the problem area addressed by modeling spans multiple disciplines, expertises and languages, emphasizing the importance of a collaboratively built and endorsed <em>worldview</em>.</p>
</div>
</div>
<div class="sect2">
<h3 id="_the_worldview">3.2. The worldview</h3>
<div class="paragraph">
<p>Both annotation and inference, as described above, require a common set of <em>ontologies</em> that define the realm of knowledge that can be integrated and conform with the foundational principles of k.LAB&#8217;s observational model. We refer to this set of ontologies as the <em>wordlview</em>, a set of k.IM projects that are automatically synchronized to all users that adopt it. A worldview is linked to each user profile and to the certificate that connects each k.LAB Node to the k.LAB network; only engines and nodes that adopt the same worldview as the user&#8217;s are seen in a k.LAB session.</p>
</div>
<div class="paragraph">
<p>As a worldview is meant to describe <em>observation</em> of reality, not reality itself, it is naturally aware of <em>scale</em>; its semantics differentiates observables not only by phenomenological nature but also by the nature of the observation process applicable to them. For example, k.LAB distinguishes <em>events</em> from <em>processes</em>, a distinction that has no real epistemological rationale (and does not exist in ontologies such as <a href="http://www.obofoundry.org/ontology/bfo.html">BFO</a>) but is fundamental from an observational perspective, as events are <em>countable</em> entities and therefore need to be instantiated, producing zero or more independent observations, before resolution, while only one instance of the same process may exist within the subject that provides a context for it. The range of scales of observation is key to the compatibility of worldviews: while a single worldview can easily address the wide range of problems that are "visible" at the scale of observation of a human observer, encompassing for example economic, ecological and social phenomena, it would be difficult to maintain meaning if that same worldview was also used to annotate problems at extremely small (e.g. quantum physics) or large (e.g. general relativity) observational scales.</p>
</div>
<div class="paragraph">
<p>The development of a worldview is a large collaborative endeavor, whose success is essential to the full fruition of the k.LAB paradigm. To date, only one worldview (the <code>im</code> worldview, for Integrated Modeling) is being developed, initially within the k.LAB team with an extended group of collaborators. This worldview currently consists of <em>Tier 1</em> namespaces, covering a set of disciplinary realms with only enough detail to enable k.LAB&#8217;s current applications. As applications of k.LAB grow, a process for the collaborative development, versioning and maintenance of the Tier 1 IM worldview will become an important area of emphasis. Tier 2 namespaces will be defined to specialize and add detail to the corresponding Tier 1 namespaces: for example, the Tier 1 <code>hydrology</code> namespace will be complemented by a project containing <code>hydrology.xxx</code> namespaces for each sub-area of hydrology needed by specialized applications. Such Tier 2 projects will be tied to user groups that each user can opt in through their user profile on the k.LAB hub, so that those users can automatically access any projects and models that require Tier 2 concepts to be understood by the system. This modular approach will enable specific user groups to control the development of the needed terminology while remaining compatible with the core concepts in Tier 1, and allow a scaled and coordinated development of the knowledge base without overwhelming those users who don&#8217;t need specialized detail. The semantic server described in the introduction will recognize the user groups and provide suggestions for annotation matching the chosen areas of expertise and level of detail.</p>
</div>
<div class="sect3">
<h4 id="_authorities">3.2.1. Authorities</h4>
<div class="paragraph">
<p>Providing semantics for identities such as taxonomic or chemical species presents a special challenge, as their number is virtually infinite: as a result, most commonly used ontologies (such as those in the <a href="http://www.obofoundry.org/">OBO foundry</a>) resort to providing <em>some</em> of the identities most likely needed by the communities of reference, but it is impossible to address all use cases with full generality, and even importing specialized ontologies (such as <a href="http://www.obofoundry.org/ontology/chebi.html">CHEBI</a> for chemical identities) risks overwhelming the inference engine with too many (and still often not enough) concepts, or creating unnecessary incompatibility stemming from the usage of equivalent concepts from different ontologies. In k.LAB, this problem is obviated through the introduction of <em>authorities</em>, a mechanism to interface with external vocabularies that enjoy broad community acceptance, fully integrated in the k.IM language. Such vocabularies are seen by contributors and users as externalized namespaces. An authoritative identity takes, in the k.IM language, the form <code>IUPAC:water</code>, easily distinguished from other concepts by its uppercase namespace identifier (a regular concept would have a lowercase namespace, e.g. <code>geography:Slope</code>). Its use in k.IM triggers validation of the concept identifier (<code>water</code>) using an online service tied to the authority (<code>IUPAC</code>), which is advertised by nodes in the k.LAB network. Upon successful validation, an identity concept is produced for the statement whose definition is identical and stable at all points of use. This mechanism allows externalizing large vocabularies (such as the IUPAC catalog of chemical species or the GBIF taxonomy identifiers) and structured specification conventions (such as the World Reference Base for soil types) that are validated and turned into stable, k.LAB-aligned semantics at the moment of their use. Another advantage of many authorities is flexibility of specification: for example, <code>IUPAC:water</code> and <code>IUPAC:H2O</code> are valid identifiers that can be used in k.IM observables as written, and translate to the same concept (the chemical identity corresponding to water, encoded internally as the standard InChl key) using a IUPAC-endorsed catalog service provided by NIH. The k.LAB stack provides content contributors with assisted search interface and intelligent editor support with inline, "as-you-type" validation and documentation. The currently supported authorities include IUPAC, GBIF, the World Reference Base soil classification formalism, and the set of UN-endorsed statistical classifications provided through the FAO <a href="https://stats-class.fao.uniroma2.it/caliper">CALIPER</a> service (the latter in development at the time of this writing).</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_learning_models">3.3. Learning models</h3>
<div class="paragraph">
<p>An important part of modeling is adapting a computation to known data, so that it can best reproduce a known output from a known set of inputs, in order to increase confidence in predicted results when the model is run with unknown inputs. The main use cases for this activity are <em>machine learning</em>, which iteratively <em>trains</em> a statistical model until it produces the best fit to known data, and <em>model calibration</em> or <em>data assimilation</em>, used to find the optimal parameterization of mechanistic models. No modeling platform would be complete without addressing these "learning" capabilities. In k.LAB, model learning exploits the separation of the resource and semantic layer and the ability to find both inputs and outputs by resolving semantics. Models introduced by the keyword <code>learn</code> instead of <code>model</code> will resolve their outputs as well as their inputs, and produce, using a specialized function, a <em>computable resource</em> with a specified URN, independent of semantics, containing the trained computation for future reuse. As an example, a minimal Bayesian suitability model to inform a land cover change model could be built using the following specification:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-kim hljs" data-lang="kim">learn landcover:LandCoverType
	observing
		@predictor distance to infrastructure:Highway,
		@predictor distance to earth:Waterway,
		@predictor distance to earth:Coastline,
		@predictor geography:Slope,
		@predictor geography:Elevation,
		@predictor count of demography:HumanIndividual,
		@predictor earth:AtmosphericTemperature in Celsius
	using im.weka.bayesnet(resource = luc.suitability);</code></pre>
</div>
</div>
<div class="paragraph">
<p>The function call after the keyword <code>using</code> is a <em>contextualizer</em> that invokes a learning process from the <a href="https://www.cs.waikato.ac.nz/ml/weka">WEKA</a> software, specifically a Bayesian learner. When run in a spatially distributed context, the above model will resolve both the output (land cover type) and all predictors in the context of execution, sample them to produce a training dataset, and pass the latter to Weka to build and train a Bayesian model, which is in turn used to produce the <code>luc.suitability</code> local resource (using the WEKA adapter) in the same project where the model is found. An interpolated map with the model&#8217;s prediction, along with a report including all metrics of fit, is also produced to ease result evaluation. The trained Bayesian network can be modified and retrained as needed using WEKA and its integration with k.LAB. When satisfactory, the trained model can be used for prediction through the URN of the trained resource:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-kim hljs" data-lang="kim">model luc.suitability as landcover:LandCoverType
	observing
		distance to infrastructure:Highway,
		distance to earth:Waterway,
		distance to earth:Coastline,
		geography:Slope,
		geography:Elevation,
		count of demography:HumanIndividual,
		earth:AtmosphericTemperature in Celsius;</code></pre>
</div>
</div>
<div class="paragraph">
<p>The above model uses the trained Bayesian classifier to produce probabilistic predictions of land cover type. With probabilistic resources such as this, an uncertainty map can also be obtained by adding the uncertainty concept if desired:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-kim hljs" data-lang="kim">model luc.suitability as landcover:LandCoverType,
		uncertainty of landcover:LandCoverType
	observing
		....</code></pre>
</div>
</div>
<div class="paragraph">
<p>Similar considerations apply to other learning algorithms such as the rest of the WEKA platform or others such as Google&#8217;s TensorFlow. The resource containing the trained model will link its inputs by name and data type, and can be published to a node for remote execution by any users just like any other resource. Similar considerations apply to the prediction of qualities within countable entities (subjects, events, relationships) that are part of the context, training a classifier using each instance and its attributes as a training sample instead of sampling a distributed dataset like in the example above.</p>
</div>
<div class="paragraph">
<p>The problem of <em>calibration</em> or <em>data assimilation</em> of numerical models can be handled in the same fashion, by linking appropriate algorithms to k.LAB. At the time of this writing, an interface to the open source <a href="http://www.openda.org/index.php">OpenDA</a> package is being investigated for future integration.</p>
</div>
</div>
<div class="sect2">
<h3 id="_sessions_and_outputs_of_contextualization">3.4. Sessions and outputs of contextualization</h3>
<div class="paragraph">
<p>Within a k.LAB session, a user or application sets a context and observes as many concepts as desired. Observations that were already made in the context automatically resolve any subsequent query for compatible concepts. At any time, the user or application can set or unset one or more <em>scenarios</em> to affect the selection of models. A scenario in k.LAB is simply a namespace whose contained models become "visible" to the system only when it is explicitly activated: when a scenario is active, its models take priority over any others to resolve their observables, potentially using other models to complete observations in case the scenario is only defined to cover a part of the context. Using scenarios, the environment within a context may be interactively defined to reflect specific hypotheses. In interactive use (for example with k.LAB Explorer) it is possible to build observation sets that use different scenarios, incrementally defining a context that reflects any desired conditions.</p>
</div>
<div class="paragraph">
<p>A context always contains the complete history of observations made, including the metadata and provenance records for all resources and models used. As dataflows are resolved and contextualized, provenance records stored along with the knowledge will be extended with all logical steps followed to compute the corresponding observations, and remain available within the context to form a complete record of how the information in it has come into existence. All this information is available interactively to the user in graphical form when using a k.LAB client, and becomes part of the set of downloadable artifacts accessible within a context. These include:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>A complete dataflow that includes all the processing steps and resources accessed to compute every observation in the context. The k.LAB runtime uses a specialized language, k.DL, to encode dataflows in a concise and reusable way; the k.DL code can be visualized (as text or as a flowchart-like diagram) and saved to a resource to reproduce the computations as needed. When saving to a resource, the k.LAB engine will compute the intersected spatial and temporal coverage of all resources and models involved, so that the dataflow can be saved along with the detailed geometry where the computations can be replicated.</p>
</li>
<li>
<p>Complete provenance information for all the resource and models used in the context. The k.LAB runtime adheres internally to the <a href="https://openprovenance.org/opm">Open Provenance Model (OPM)</a> conventions, which are central to the layout of the internal class structure. An API call to extract the OPM-compatible provenance graph for a context is expected in version 1.0.</p>
</li>
<li>
<p>A tree of observations, each of which can be downloaded to the file formats supported by the configured adapters according to the spatial and temporal dimensions in the context. For example, an observations of a numerical or categorical quality (<em>state</em>) can be downloaded to a CSV file if scalar or distributed only in time, to a raster map (e.g. GeoTIFF or ArcGIS format) if spatially distributed on a grid, or to an archive file with a map per timestep if distributed in both space and time. Observations of subjects (e.g. the lakes in the context) can be downloaded to database files, including ESRI shapefiles when the objects have a spatial coverage.</p>
</li>
<li>
<p>The user may request, in lieu of individual observations, <em>views</em> that contextualize a specified concept and summarize the result in complex ways, such as tables or graphs. Such views also become part of the context along with all the observations made to compute them. These can be exported as spreadsheets or other appropriate formats. The table generation features in k.LAB refer to observables using pure semantics, enable flexible specification of aggregations and allow users or modelers to build sophisticated and complex reports with very short specifications. Tables are prominently used, for example, in Natural Capital Accounting applications such as <a href="https://seea.un.org/content/aries-for-seea">ARIES for SEEA</a>.</p>
</li>
<li>
<p>As models are computed by the system, a user-readable, structured <em>report</em> is generated and incorporated within the context. The documentation features in k.LAB rely on a simple template language that can be associated to models in k.IM code and allows modelers to link documentation templates to events that are triggered during contextualization (for example, initialization or termination) and report sections such as introduction, methods, results and discussion. The k.Modeler IDE contains specialized support for writing and organizing documentation in k.LAB projects. By using the Markdown language supplemented with template directives, structured text can be inserted in the generated documentation along with figures, tables, cross-references and bibliographic citations. The k.LAB engine incrementally assembles the report as new models are contextualized, producing a unified document that can be tailored to the context and to the actual results obtained using conditional template directives and context-aware text substitutions. This feature enables the production of very complete textual reports that can be downloaded as PDF through the clients or the API.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The set of outputs obtained and visualized during a k.LAB session ensures the transparency and communicability of the results to a degree never seen in a modeling platform. In some situations, even the paths <em>not</em> taken by the resolver can be documented, which may be relevant when multiple resources with close rankings are available in resolution. The possibility of producing <em>digitally signed artifacts</em> including all of outputs, report, dataflow and full provenance graph, plus (if needed) verifying and documenting the provenance and the peer review status of all resources and models involved, opens the way to the production and the verification of <em>endorsed</em> artifacts when the system is used to produce information from official institutional applications, or in situations when use of the result can have critical consequences in decision-making.</p>
</div>
</div>
<div class="sect2">
<h3 id="_extending_the_runtime_system">3.5. Extending the runtime system</h3>
<div class="paragraph">
<p>The k.LAB engines and nodes can be extended at the software level to provide new adapters, contextualizers, or other functionalities to support new integrations or resource types. A mechanism to produce <em>components</em> that can be used as plug-ins uses well-defined and documented points of extension in the Java class structure, and is supported by Maven archetypes for convenient project setup, building and deployment. The design of the server components is highly modular, and each existing resource adapter, external package integration (such as the WEKA machine learning software) or functionality extension is written as a component that can be deployed to nodes and services. The contextualization runtime, which executes the resolved dataflows and can load them from a stored k.DL specification, can itself be swapped with an alternative execution runtime if wished, for example to support different runtime platforms (e.g. to run contextualizations on distributed file systems). The default runtime coming with k.LAB is parallelized and multi-threaded, capable of handling concurrent sessions owned by different users and optimizing the use of RAM to enable large-scale simulations.</p>
</div>
</div>
<div class="sect2">
<h3 id="_integrating_external_models">3.6. Integrating external models</h3>
<div class="paragraph">
<p>Integration of k.LAB with existing models can proceed in two directions. By using the k.LAB API from within an existing model, the inputs of the model can be satisfied using semantic resolution, streamlining and simplifying data access from a largely unmodified model. By contrast, deep integration of a model into the k.LAB framework normally requires significant redesign, but can make the model and its components available to k.LAB users and other models as part of the k.LAB ecosystem, greatly enhancing its original value.</p>
</div>
<div class="sect3">
<h4 id="_integrating_k_lab_into_existing_models">3.6.1. Integrating k.LAB into existing models</h4>
<div class="paragraph">
<p>In this integration configuration, the REST API of a k.LAB engine (or cluster of engines) can be used, after authentication, from within an independent application to enable the use of the k.LAB semantic network without integrating the application itself in k.LAB. Applications that formerly loaded their outputs from the filesystem, relying on configuration files or interactive forms, would at this point simply define the geometry of interest and the semantics for their desired inputs. This paradigm does not make the application itself or its outputs available to k.LAB users, and is therefore less valuable from an integration perspective, but it can constitute a first incentive to more productive integrations. At the time of this writing no language-specific client libraries have been written to ease the client use of k.LAB from, e.g., Python or Javascript applications, but the direct use of the REST api remains possible.</p>
</div>
</div>
<div class="sect3">
<h4 id="_integrating_existing_models_into_k_lab">3.6.2. Integrating existing models into k.LAB</h4>
<div class="paragraph">
<p>Integration of existing models so that they become part of the k.LAB environment is possible in several ways. The preferred strategy is to break down the logical data flow inside a model into components that describe each individual concept within the model, then revise each of these components as independent models. From an interoperability perspective, this provides the greatest return, by ensuring the full integration of any internal feedbacks and sensitivity to changing boundary conditions. However, this approach also requires the most work to rethink each models internal logic, as most models have been written with specific conventions, if not even conditions of use, in mind that remain unwritten. This often mandates the generalization of the context of use of each model - for instance, generalizing a hydrologic model originally designed to run at an annual time scale to run on at more flexible time steps while remaining faithful to (time-agnostic) underlying physical processes to the degree possible. This may be difficult and time-consuming, particularly when the original implementation of the model is unclear, poorly documented, or logically inconsistent.</p>
</div>
<div class="paragraph">
<p>Preexisting models usually consist of highly connected networks of computations that are difficult to break into components to best fit an interoperable, semantic modeling paradigm. Yet, tightly defined and well-focused models can be used as "functions" when (1) their inputs and outputs are well-defined semantically, (2) data needs are clearly described, and (3) appropriate spatial/temporal scales for their use are provided. This is usually most convenient when their internal logic is complex and difficult to break up.</p>
</div>
<div class="paragraph">
<p>Three possible strategies to make pre-packaged models interoperable with k.LAB include:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Wrapping them into web services and connecting them to an API capable of mediating with k.LABs data transfer format. The model will be connected using the "remote" k.LAB adapter, which uses a REST API and can therefore be coupled to model services written in any language. This alternative requires little further work on the models themselves, but requires a "bridge" API for the host programming language to facilitate integration with the k.LAB interface. At the time of this writing bridge APIs exist only for Java, but those for other languages will be developed based on demand.</p>
</li>
<li>
<p>Creating a k.LAB contextualizer as an extension that gathers input from the k.LAB environment, passes it to the model for computation, and serves the outputs back. This does not require the mediation of a web service and thus entails more direct connections to the model code. The model may be connected at the code level, which is easiest in Java but can be supported by adapters for other languages.  Alternatively, the model may be run as an external application,  requiring no coding besides that needed to prepare inputs and gather outputs (this strategy is likely to be computationally inefficient, particularly for dynamic models that require independent runs over multiple time steps). Running as an external application may prove impossible when internal feedbacks must be connected to boundary conditions handled by the k.LAB environment, and while tempting because of the low development barrier, these kinds of solutions tend to have a limited useful life.</p>
</li>
<li>
<p>Isolating the core algorithms in the model and reimplementing them in code as contextualizers using the native k.LAB API. This middle-ground integration strategy neither reuses the original code as-is nor requires a full semantic annotation effort to fully integrate them. This approach is usually the easiest way to bring in existing logics without a major effort. As k.LAB takes care of I/O, data transformation and preparation, data flow, spatial and temporal addressing, and visualization, the rewrite usually only has to cover a small percentage of any original stand-alone model code, normally between 10 and 30%.</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>Overall, strategy 1 is the most generalizable solution (i.e., more bridge APIs would facilitate the integration of more external models with k.LAB). Strategy 3 is a practical solution when a smaller number of models are targeted for integration. Strategy 2 is the most <em>ad hoc</em>, with several key limitations; as such it can be seen as a generally less desirable strategy.</p>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_the_reactivity_layer_behaviors_and_applications">4. The reactivity layer: behaviors and applications</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The semantic modeling approach discussed so far is designed to construct simulated worlds, using the best available data and models, based on their logical description. The observations that compose these worlds can be construed as the outputs of the underlying modeling, and will incorporate any dynamic behavior that can be stated along with the logical description in k.IM models and contextualizers - typically, process models whose behavior is known in advance. While many phenomena can be described satisfactorily within this paradigm, others - namely, those where <em>events</em> triggered by specific conditions cause modifications in the structure of the system - can not. Addressing these aspects of <em>agency</em> and <em>reactivity</em> is the purpose of the k.LAB reactivity layer.</p>
</div>
<div class="paragraph">
<p>The reactivity layer contains a collection of <em>behaviors</em>, i.e. specifications of how any agent (the observations in a context, the context itself, or even the k.LAB session or the user owning it) can react to conditions that come to pass during the course of contextualization. The reactivity layer is key to developing complex, distributed <em>agent-based models</em> that are fully semantically aware, and allows modelers to build interactive visualizations and applications when the behavior is applied to a session. All behaviors take the form of code specified in the k.Actors language, supported by the k.Modeler IDE and used to define behaviors for observations, test cases, batch computations, UI components and interactive applications.</p>
</div>
<div class="paragraph">
<p>The k.Actors language has a simple, minimal syntax that belies a complex and powerful model of execution. Both k.IM and k.Actors draw their syntax from the English language; if the k.IM language is concerned with representing what observations <em>are</em> and how they are computed, k.Actors is concerned with representing how they <em>behave</em>. For this reason, the linguistic realm of k.IM is that of nouns, adjectives and adverbs, while k.Actors deals mostly with <em>verbs</em>. Compared with k.IM, which is optimized to be usable at the simplest levels by modelers without programming experience, k.Actors reads less like English than k.IM and is more suitable to experienced programmers. An annotated example is provided below, with no in-depth discussion, to give a flavor of the language:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-kactors hljs" data-lang="kactors">behavior demo.restaurant
    "Invite a friend to dinner and if accepted, choose a restaurant in the context"

// the main action will be triggered when the behavior is loaded
action main:
	invite("friend@email.com"): "OK" -&gt; choose({infrastructure:Restaurant}): reserve($)

action invite(friend):
	email("Hi, shall we go out for dinner tonight?", address=friend):
		answer -&gt; sentiment.classify(answer, {im:Outcome}): (
						{im:Positive} -&gt; email("Great", address=[answer.replyAddress]),  "OK"
					    {im:Negative} -&gt; email("Sorry", address=[answer.replyAddress]), "NO")</code></pre>
</div>
</div>
<div class="paragraph">
<p>In the code above, two <em>actions</em> are defined, each composed of one statement that calls other actions and specifies a chain of events triggered when each of them "responds" (<em>fires</em>). In action <code>main</code>, the verb <code>invite</code> is called, passing an email address as a parameter. The call, defined later in the code, results in an email being sent and its response being processed, eventually firing back a status code ("OK" or "NO") to the calling action. The OK code triggers the choice of a restaurant in the context and its booking when found.</p>
</div>
<div class="paragraph">
<p>In k.Actors&#8217;s concurrent mode of execution, actions may cause events (<em>fire</em>) zero or more times, and those events can be captured by the code that called the action using the <code>:</code> and <code>&#8594;</code> operators. When executing the code, the runtime starts each action and immediately moves on, without waiting for it to fire unless synchronous execution is forced. If the ':' operator follows the call, the actor running the behavior readies itself to process events fired by it, whenever they happen, which may be any time (or never) as long as the actor is "alive". The data associated with the event are matched to the expression that precedes the arrow operator <code>&#8594;</code>, and if the match succeeds the code following the operator is executed.</p>
</div>
<div class="paragraph">
<p>Behaviors written in k.Actors can be, in the simplest cases, bound to the observations created by models using k.IM code:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-kim hljs" data-lang="kim">@bind(city.demo.behavior, select=[self.population &gt; 100000])
model each klab:osm:point:city as infrastructure:City;</code></pre>
</div>
</div>
<div class="paragraph">
<p>which will bind the <code>city.demo.behavior</code> behavior to any city whose population is higher than 100,000. Behaviors can also be bound to observations by actions in other behaviors, based on semantic type or other conditions, or directly from within code specified in k.IM models.</p>
</div>
<div class="paragraph">
<p>In the forthcoming version 1.0 of k.LAB, observations that are part of contexts in remote k.LAB engines will be accessible by prepending the URL of a connected context to the identifier of each observation; this opens the door to <em>distributed real-time simulations</em> whose agents can affect each other remotely. The paradigm of distributed, collaborative modeling enacted through the semantic layer can therefore, through the reactivity layer, extend to one where already initialized simulated worlds can interact with each other, building large-scale, multi-server simulations that can track events happening at each side. Institutions with expertise in tracking and predicting real-world phenomena of a particular category can make their digital "peers" available for other models to use. In the reference k.LAB implementation, the actor facilities utilize open source technical solutions originally developed for the <a href="https://en.wikipedia.org/wiki/Internet_of_things">Internet of Things</a>, capable of handling the functionalities described to build an "internet of observations" in support of real-time, better informed decision.</p>
</div>
<div class="sect2">
<h3 id="_user_side_applications">4.1. User-side applications</h3>
<div class="paragraph">
<p>Within the k.LAB runtime, the software "agents" capable of receiving a behavior are not only the observations built within sessions, but also the sessions themselves and the users that own them. This opens the door to the application of behaviors for purposes beyond the modeling of agents within simulations. In particularly, when a behavior is applied to a user session, the session can be seen as an <em>application</em> whose actions are performed through the client software, and the consequences of which can trigger observations or other events. Coupled with the ability of k.Actors to interact with the runtime and use semantics for queries, this feature enables a very intuitive way to build user applications in k.Actors. The web client, k.LAB Explorer, is fully equipped to respond to events triggered by actions by creating user interface components (such as buttons, text fields, lists etc.); users interacting with these components will "fire" events that are sent back to the k.Actors runtime for processing. The resulting interactive application is typically very quick to build. For example, the following code</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-kactors hljs" data-lang="kactors">app example.ui.minimal
	"A simple demo of UI definition with k.Actors."
	description "This application demonstrates some basic UI widgets and interaction with the
				 k.LAB runtime environment. An 'app' is a behavior applied to a k.LAB session."
	style default with #{
    	font-size: '0.85em'
	}

@left
action main:

	set outputs []

	%%%
		**Markdown** and HTML text widget between matching percent markers (\%\%\%).
		Write any *markdown* in this field to put text in the UI. The :scroll and
		:collapse attributes control the appearance.
	%%% :scroll :collapse

	/*
	 * Groups in parentheses become divisions in the UI and can be styled with attributes
	 */
	(
	  button("Set context to France and observe Elevation"):
	  		context(im.countries.france): france -&gt; france.observe({geography:Elevation}): outputs.add($)
	  button("Observe vegetation C storage in the current context"):
	  		submit({ecology:Vegetation chemistry:Carbon im:Mass}): outputs.add($)
	) :hbox :name "Sample observations (click to observe)"

	/*
	 * a final button enables downloading all the observations accumulated when pressing the
	 * buttons above.
	 */
	button ("Maps" #mapdownload :tooltip "Download all observations as a zip file"):
		(
			mapdownload.waiting
			pack(outputs): (
				url -&gt; (
					mapdownload.reset
					download(url, filename="data.zip")
				)
				error -&gt; mapdownload.error(:timeout 1000)
			)
		 )</code></pre>
</div>
</div>
<div class="paragraph">
<p>creates a demonstrational application that will show a buttons to make observations and collects the results in an array so that the corresponding data can be downloaded in one action. The UI will appear in k.LAB Explorer. Using modular components, interfaces such as the code for the <a href="https://seea.un.org/content/aries-for-seea">ARIES for SEEA</a> application can be build by minimally trained programmers in a short time (the code for the above application at the time of this writing is only about 300 lines long).</p>
</div>
<div class="paragraph">
<p>In addition to these usages, k.Actors is used as a scripting language to automate repetitive tasks (for example to build global high-resolution maps describing a single observable, by computing it in multiple local contexts with fully customized model resolution in each) and to build test suites for all aspects of k.LAB.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_current_status">5. Current status</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The k.LAB software stack is currently in version 0.11, and no feature-completion or API stability is guaranteed until version 1.0 is reached. According to funding and community uptake, this state is expected to be reachable in the period 2022 to 2023. The current status can be briefly summarized as follows:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The software can be considered at production levels for the functions that support applications such as the general k.LAB explorer for the <a href="https://aries.integratedmodelling.org">ARIES project</a> and specialized apps like ARIES for SEEA. Visualization and reporting are at near-feature completion for current uses.</p>
</li>
<li>
<p>Installable containers for nodes and engines are well-developed and used regularly, although few partner nodes besides the central team and the UN exist, and frequent upgrades are necessary.</p>
</li>
<li>
<p>Feature completion vs. the planned set of features is at about 90%, enough for current applications but still needing work for full-scale agent-based modeling, real-time applications and other types of use.</p>
</li>
<li>
<p>Resource adapters are available for most important data formats, services and protocols. Assisted user interfaces to contribute data and models are limited for now to the modeling environment k.Modeler, which is functional but not suitable for non-technical users. More data submission methods and interfaces are to be developed in 2021 to support use by countries and institutions involved in ongoing projects.</p>
</li>
<li>
<p>Besides an initial grant from the US National Science Foundation, k.LAB has seen a limited but reliable funding stream for its development, with low- to mid-levels of financing but a relatively high stability. The current preference is for a partnership model rather than individual grants, as continuity and talent retention are more important to guarantee ultimate success than large investments and one-off funding.</p>
</li>
<li>
<p>The REST API is currently optimized for applications and use "within" the system using its own clients based on k.LAB Explorer: more discussion will be needed before a stable API specification, to be used independently, is published.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Technical inquiries on k.LAB should be addressed to <a href="mailto:info@integratedmodelling.org">info@integratedmodelling.org</a>.</p>
</div>
</div>
</div>
</div>
<div id="footnotes">
<hr>
<div class="footnote" id="_footnotedef_1">
<a href="#_footnoteref_1">1</a>. See <a href="https://f1000research.com/articles/6-686"><em>Villa F, Balbi S, Athanasiadis IN and Caracciolo C. Semantics for interoperability of distributed data and models: Foundations for better-connected information</em></a> for (slightly outdated) details on the phenomenological model underlying k.LAB&#8217;s semantics.
</div>
<div class="footnote" id="_footnotedef_2">
<a href="#_footnoteref_2">2</a>. The <code>biology:Eucalyptus</code> species identity, used here for simplicity, would in reality be handled through a taxonomic authority: see the section <em>Authorities</em> below for details.
</div>
</div>
<div id="footer">
<div id="footer-text">
</div>
</div>
<script src="C:/Users/Ferd/git/klab/docs/etc/highlightjs4klab/build//highlight.min.js"></script>
<script src="C:/Users/Ferd/git/klab/docs/etc/highlightjs4klab/build//languages/kim.min.js"></script>
<script src="C:/Users/Ferd/git/klab/docs/etc/highlightjs4klab/build//languages/java.min.js"></script>
<script>
if (!hljs.initHighlighting.called) {
  hljs.initHighlighting.called = true
  ;[].slice.call(document.querySelectorAll('pre.highlight > code')).forEach(function (el) { hljs.highlightBlock(el) })
}
</script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  messageStyle: "none",
  tex2jax: {
    inlineMath: [["\\(", "\\)"]],
    displayMath: [["\\[", "\\]"]],
    ignoreClass: "nostem|nolatexmath"
  },
  asciimath2jax: {
    delimiters: [["\\$", "\\$"]],
    ignoreClass: "nostem|noasciimath"
  },
  TeX: { equationNumbers: { autoNumber: "none" } }
})
MathJax.Hub.Register.StartupHook("AsciiMath Jax Ready", function () {
  MathJax.InputJax.AsciiMath.postfilterHooks.Add(function (data, node) {
    if ((node = data.script.parentNode) && (node = node.parentNode) && node.classList.contains("stemblock")) {
      data.math.root.display = "block"
    }
    return data
  })
})
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/MathJax.js?config=TeX-MML-AM_HTMLorMML"></script>
</body>
</html>