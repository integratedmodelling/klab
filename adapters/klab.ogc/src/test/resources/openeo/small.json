{
  "process_graph": {
    "loadresult1": {
      "process_id": "load_result",
      "arguments": {
        "id": "local_copy.tif"
      }
    },
    "reducedimension1": {
      "process_id": "reduce_dimension",
      "arguments": {
        "data": {
          "from_node": "loadresult1"
        },
        "dimension": "bands",
        "reducer": {
          "process_graph": {
            "runudf1": {
              "process_id": "run_udf",
              "arguments": {
                "context": {
                  "eunis_level": {
                    "from_parameter": "eunis_level"
                  }
                },
                "data": {
                  "from_parameter": "data"
                },
                "runtime": "Python",
                "udf": "\nfrom typing import Dict, List, Tuple, Union\nimport xarray as xr\nimport subprocess\nimport numpy as np\nimport os\nfrom openeo.udf import inspect\nimport sys\n\nsys.path.append(\"onnx_deps\") \nimport onnxruntime as ort\n\n# Author: AM1729\n\n## This udp is very similar to the one originally written by VITO team\n## except that this would return directly the Tiff with Max Probable Classes\n## and not number of Tiff or Multi Band Geo Raster\n\n\nMODEL_URLS = ['https://s3.waw3-1.cloudferro.com/swift/v1/ecdc-waw3-1-ekqouvq3otv8hmw0njzuvo0g4dy0ys8r985n7dggjis3erkpn5o/models/Level1_class-0_129predictors_v1.onnx', \n                  'https://s3.waw3-1.cloudferro.com/swift/v1/ecdc-waw3-1-ekqouvq3otv8hmw0njzuvo0g4dy0ys8r985n7dggjis3erkpn5o/models/Level2_class-C_71predictors_v1.onnx', 'https://s3.waw3-1.cloudferro.com/swift/v1/ecdc-waw3-1-ekqouvq3otv8hmw0njzuvo0g4dy0ys8r985n7dggjis3erkpn5o/models/Level2_class-D_68predictors_v1.onnx', \n                  'https://s3.waw3-1.cloudferro.com/swift/v1/ecdc-waw3-1-ekqouvq3otv8hmw0njzuvo0g4dy0ys8r985n7dggjis3erkpn5o/models/Level2_class-E_85predictors_v1.onnx', 'https://s3.waw3-1.cloudferro.com/swift/v1/ecdc-waw3-1-ekqouvq3otv8hmw0njzuvo0g4dy0ys8r985n7dggjis3erkpn5o/models/Level2_class-F_90predictors_v1.onnx', \n                  'https://s3.waw3-1.cloudferro.com/swift/v1/ecdc-waw3-1-ekqouvq3otv8hmw0njzuvo0g4dy0ys8r985n7dggjis3erkpn5o/models/Level2_class-G_164predictors_v1.onnx', 'https://s3.waw3-1.cloudferro.com/swift/v1/ecdc-waw3-1-ekqouvq3otv8hmw0njzuvo0g4dy0ys8r985n7dggjis3erkpn5o/models/Level2_class-H_65predictors_v1.onnx', \n                  'https://s3.waw3-1.cloudferro.com/swift/v1/ecdc-waw3-1-ekqouvq3otv8hmw0njzuvo0g4dy0ys8r985n7dggjis3erkpn5o/models/Level2_class-I_50predictors_v1.onnx', 'https://s3.waw3-1.cloudferro.com/swift/v1/ecdc-waw3-1-ekqouvq3otv8hmw0njzuvo0g4dy0ys8r985n7dggjis3erkpn5o/models/Level2_class-J_62predictors_v1.onnx', 'https://s3.waw3-1.cloudferro.com/swift/v1/ecdc-waw3-1-ekqouvq3otv8hmw0njzuvo0g4dy0ys8r985n7dggjis3erkpn5o/models/Level2_class-X_54predictors_v1.onnx', \n                  'https://s3.waw3-1.cloudferro.com/swift/v1/ecdc-waw3-1-ekqouvq3otv8hmw0njzuvo0g4dy0ys8r985n7dggjis3erkpn5o/models/Level3_class-C1_62predictors_v1.onnx', 'https://s3.waw3-1.cloudferro.com/swift/v1/ecdc-waw3-1-ekqouvq3otv8hmw0njzuvo0g4dy0ys8r985n7dggjis3erkpn5o/models/Level3_class-C3_62predictors_v1.onnx', 'https://s3.waw3-1.cloudferro.com/swift/v1/ecdc-waw3-1-ekqouvq3otv8hmw0njzuvo0g4dy0ys8r985n7dggjis3erkpn5o/models/Level3_class-E1_50predictors_v1.onnx', \n                  'https://s3.waw3-1.cloudferro.com/swift/v1/ecdc-waw3-1-ekqouvq3otv8hmw0njzuvo0g4dy0ys8r985n7dggjis3erkpn5o/models/Level3_class-E2_50predictors_v1.onnx', 'https://s3.waw3-1.cloudferro.com/swift/v1/ecdc-waw3-1-ekqouvq3otv8hmw0njzuvo0g4dy0ys8r985n7dggjis3erkpn5o/models/Level3_class-E3_50predictors_v1.onnx', 'https://s3.waw3-1.cloudferro.com/swift/v1/ecdc-waw3-1-ekqouvq3otv8hmw0njzuvo0g4dy0ys8r985n7dggjis3erkpn5o/models/Level3_class-E4_50predictors_v1.onnx', \n                  'https://s3.waw3-1.cloudferro.com/swift/v1/ecdc-waw3-1-ekqouvq3otv8hmw0njzuvo0g4dy0ys8r985n7dggjis3erkpn5o/models/Level3_class-E5_50predictors_v1.onnx', 'https://s3.waw3-1.cloudferro.com/swift/v1/ecdc-waw3-1-ekqouvq3otv8hmw0njzuvo0g4dy0ys8r985n7dggjis3erkpn5o/models/Level3_class-F2_60predictors_v1.onnx', 'https://s3.waw3-1.cloudferro.com/swift/v1/ecdc-waw3-1-ekqouvq3otv8hmw0njzuvo0g4dy0ys8r985n7dggjis3erkpn5o/models/Level3_class-F3_50predictors_v1.onnx', 'https://s3.waw3-1.cloudferro.com/swift/v1/ecdc-waw3-1-ekqouvq3otv8hmw0njzuvo0g4dy0ys8r985n7dggjis3erkpn5o/models/Level3_class-G1_74predictors_v1.onnx', 'https://s3.waw3-1.cloudferro.com/swift/v1/ecdc-waw3-1-ekqouvq3otv8hmw0njzuvo0g4dy0ys8r985n7dggjis3erkpn5o/models/Level3_class-G3_98predictors_v1.onnx', 'https://s3.waw3-1.cloudferro.com/swift/v1/ecdc-waw3-1-ekqouvq3otv8hmw0njzuvo0g4dy0ys8r985n7dggjis3erkpn5o/models/Level3_class-H2_50predictors_v1.onnx', 'https://s3.waw3-1.cloudferro.com/swift/v1/ecdc-waw3-1-ekqouvq3otv8hmw0njzuvo0g4dy0ys8r985n7dggjis3erkpn5o/models/Level3_class-H3_50predictors_v1.onnx']\n\nLOCAL_MODEL_DIR = \"am1729/models\"\n\n\nONNX_LEVELS_AND_CLASS_MAP = {\n    1: [\"30000\", \"40000\", \"50000\", \"60000\", \"70000\", \"80000\", \"90000\", \"100000\", \"110000\"],\n\n    2: [\"30100\", \"30200\", \"30300\",\n        \"40100\",\"40200\",\"40400\",\"40500\",\n        \"50100\", \"50200\", \"50300\" ,\"50400\", \"50500\" ,\"50600\",\n        \"60200\", \"60300\", \"60400\", \"60900\", \"61100\", \n        \"70100\", \"70200\", \"70300\",\"70400\", \n        \"80200\", \"80300\", \n        \"90100\", \"90200\", \n        \"100100\", \"100200\", \"100300\", \"100400\", \"100600\", \n        \"110400\", \"110700\", \"110800\", \"110900\", \"112100\"]\n}\n\nNODE_CODE_MAP = {}\n\nclass EunisClassification():\n\n    def __init__(self, code:str=None, child:List=[]):\n        self.code = code\n        self.child = child\n    \n    \n    def create_heirarchy():\n        root = EunisClassification(code=\"U\")\n        level1 = ONNX_LEVELS_AND_CLASS_MAP.get(1, [])\n        level2 = ONNX_LEVELS_AND_CLASS_MAP.get(2, [])\n        level3 = ONNX_LEVELS_AND_CLASS_MAP.get(3, [])\n\n        level1Child = []\n        for classCode in level1:\n            e = EunisClassification(code = classCode)\n            NODE_CODE_MAP[classCode] = e\n            level1Child.append(e)\n            level2Child = []\n            for classCodeLev2 in level2:\n                if classCodeLev2[:2] == classCode[:2]:\n                    lev2 = EunisClassification(code = classCodeLev2)\n                    NODE_CODE_MAP[classCodeLev2] = lev2\n\n                    level2Child.append(lev2)\n                    level3Child = []\n                    for classCodeLev3 in level3:\n                        if classCodeLev2[:3] == classCodeLev3[:3]:\n                            lev3 = EunisClassification(classCodeLev3)\n                            NODE_CODE_MAP[classCodeLev3] = lev3\n                            level3Child.append(lev3)\n\n                    lev2.child = level3Child\n\n            e.child = level2Child\n        \n        root.child = level1Child\n        return root\n\n            \ndef get_level_model_map()->Dict[str, str]:\n    '''\n    Map of a EUNIS Class Rep and ONNX Model\n\n    Gets Level to ONNX Models Map. \n    We would call onnx.load and would save the model in memory\n    '''\n\n    eunis_class_model_map:Dict[str, str] = {}\n    \n    for modelUrl in MODEL_URLS:\n        local_path = modelUrl.split(\"/\")[-1]\n        cmd = [\"curl\",\n                \"-o\", \n                LOCAL_MODEL_DIR + \"/\" + local_path,\n                modelUrl\n                ]\n        subprocess.run(cmd, capture_output=True, text=True)\n        _, metadata = load_onnx_model(f'{LOCAL_MODEL_DIR}/{local_path}')\n\n        output_features = metadata.get(\"output_features\")\n        \n        ## Store the Output Feature like 300000, 30101... to the model that \n        ## Predicts it. The value is actually the path top the stored ONNX Model\n        for output_feature in output_features:\n            eunis_class_model_map[output_feature] = LOCAL_MODEL_DIR + \"/\" + local_path\n\n    return eunis_class_model_map\n\n\ndef preprocess_input(input_xr: xr.DataArray,\n                     ort_session: ort.InferenceSession) -> Tuple[np.ndarray, Tuple[int, int, int]]:\n    \"\"\"\n    Preprocesses input data for model inference using an ONNX runtime session. This\n    function takes an xarray DataArray, rearranges its dimensions, and reshapes its\n    values to match the input requirements of the ONNX model specified by the given\n    ONNX InferenceSession.\n\n    :param input_xr: Input data in the format of an xarray DataArray. The expected\n        dimensions are \"y\", \"x\", and \"bands\", and the order of the dimensions will\n        be transposed to match this requirement.\n    :param ort_session: ONNX runtime inference session that specifies the model for\n        inference. Used to determine the required input shape of the model.\n    :return: A tuple containing:\n        - A numpy array formatted to fit the input shape of the ONNX model.\n        - The original shape of the input data as a tuple with the transposed \"y\",\n          \"x\", and \"bands\" dimensions.\n    \"\"\"\n    input_xr = input_xr.transpose(\"y\", \"x\", \"bands\")\n    input_shape = input_xr.shape\n    input_np = input_xr.values.reshape(-1, ort_session.get_inputs()[0].shape[1])\n    return input_np, input_shape\n\n\ndef find_max_class_from_dict(input_dict: Dict[str, float]) -> str:\n    \"\"\"\n    Finds the class with the maximum probability from a dictionary of classes and their\n    corresponding probabilities. The function returns the class name (key) that has the\n    highest probability value.\n\n    :param input_dict: A dictionary where keys are class names and values are their\n        corresponding probabilities.\n    :return: The class name (key) with the maximum probability value.\n\n    Note: Executing the ONNX Models also gives us the Dominant Habitat Class directly as a list, as the first output.\n    But, some of the outputs are incosistent and raised to VITO team here: https://chat.integratedmodelling.org/group/dsFnTgb3ti5ynCYhR?msg=gXCDqLjzJEafojurj\n    Point B. \n    \"\"\"\n    m = max(input_dict.values())\n    for k in input_dict.keys():\n        if input_dict[k] == m:\n            return k\n        \n    return \"\"\n\ndef run_inference(input_np: np.ndarray, ort_session: ort.InferenceSession)-> List[str]:\n    \"\"\"\n    Executes inference using an ONNX Runtime session and input numpy array. This function\n    constructs the input data for the ONNX runtime, runs the session, and extracts the\n    output probabilities as list of dictionaries.\n\n    :param input_np: Numpy array containing the input tensor data for the inference.\n    :param ort_session: ONNX Runtime inference session object used to execute the model.\n    :return: A list of dictionaries where each dictionary maps string labels to their\n        corresponding probability values for each sample, as obtained from the model's output.\n    \"\"\"\n    ort_inputs = {ort_session.get_inputs()[0].name: input_np}\n    ort_outputs = ort_session.run(None, ort_inputs)\n    max_class_list = [find_max_class_from_dict(item) for item in ort_outputs[1]]\n    return max_class_list\n\n\ndef load_onnx_model(path:str):\n\n    # Initialize the ONNX Runtime session\n    inspect(message=f\"Initializing ONNX Runtime session for model at {path}...\")\n    ort_session = ort.InferenceSession(path, providers=[\"CPUExecutionProvider\"])\n\n    # Extract metadata\n    model_meta = ort_session.get_modelmeta()\n\n    input_features = model_meta.custom_metadata_map.get(\"input_features\", \"\").split(\",\")\n    input_features = [band.strip() for band in input_features]\n\n    output_features = model_meta.custom_metadata_map.get(\"output_features\", \"\").split(\",\")\n    output_features = [band.strip() for band in output_features]\n\n    metadata = {\n        \"input_features\": input_features,\n        \"output_features\": output_features,\n    }\n\n    inspect(message=f\"Successfully extracted features from model at {path}...\")\n    return ort_session, metadata\n\n\n\ndef apply_datacube(cube: xr.DataArray, context: Dict) -> xr.DataArray:\n    '''\n    Entrypoint for the UDF. \n    It takes in EUNIS Level for which the max possible class is needed to be found \n    based on the probability.\n    '''\n     \n    cube = cube.fillna(0)\n    cube = cube.astype('float32')\n\n    os.makedirs(LOCAL_MODEL_DIR, exist_ok=True)\n\n    eunis_level = context.get(\"eunis_level\")\n    if eunis_level not in [1, 2, 3]:\n        raise ValueError(\"Eunis Level must be one of 1,2 or 3\")\n    \n    eunis_class_model_map = get_level_model_map()\n    root = EunisClassification.create_heirarchy()\n\n    ## apply eunis habitat mapping sequentially\n    ## level 1:\n\n    onnx_model_path = eunis_class_model_map[root.child[0].code]\n    onnx_inference_session, metadata = load_onnx_model(onnx_model_path)\n    input_features = metadata.get(\"input_features\")\n    inspect(f\"Found the length of Input Features: {str(len(input_features))}\")\n    subsampled_cube = cube.sel(bands = input_features) ## run inference using the input bands\n\n    processed_input, input_shape = preprocess_input(subsampled_cube, onnx_inference_session)\n    level1_cube = run_inference(processed_input, onnx_inference_session)\n    level1_cube_narray = np.array(level1_cube).T.reshape(input_shape[0], input_shape[1])\n    level1_datacube = xr.DataArray(\n            level1_cube_narray,\n            dims = [\"y\", \"x\"],\n            coords={\n                'y': subsampled_cube.coords['y'],\n                'x': subsampled_cube.coords['x']\n            }\n    )\n    if eunis_level == 1:\n        return level1_datacube\n    \n    level1_classes_found:Dict[str,int] = {}\n    master_cube = None\n\n    for _, level1Class in enumerate(level1_cube):\n        if level1_classes_found.get(level1Class, -1) != -1:\n            continue\n        else:\n            level1_classes_found[level1Class] = 1\n            parent = NODE_CODE_MAP[str(level1Class)]\n            child = parent.child\n\n            ## since all the children of a parent are always explained by one model\n            onnx_model_path = eunis_class_model_map[child[0].code]\n\n            onnx_inference_session, metadata = load_onnx_model(onnx_model_path)\n            input_features = metadata.get(\"input_features\")\n            inspect(f\"Found the length of Input Features: {str(len(input_features))}\")\n\n            subsampled_cube = cube.sel(bands = input_features) ## run inference using the input bands\n            processed_input, input_shape = preprocess_input(subsampled_cube, onnx_inference_session)\n            level2_cube_class = run_inference(processed_input, onnx_inference_session)\n            level2_cube_narray = np.array(level2_cube_class).T.reshape(1, input_shape[0], input_shape[1])\n\n            band_datarray = xr.DataArray(level2_cube_narray, \n                                                                dims = [\"bands\", \"y\", \"x\"], \n                                                                coords={\n                                                                'y': subsampled_cube.coords['y'],\n                                                                'x': subsampled_cube.coords['x'],\n                                                                'bands': [level1Class],\n                                                            })\n            \n\n            if master_cube is None:\n                master_cube = band_datarray\n            else:\n                master_cube = xr.concat([master_cube, band_datarray], dim = \"bands\")\n            \n    master_cube.sortby(\"bands\", ascending=True)\n    level2_datacube = master_cube.sel(bands= level1_datacube, drop = True)\n\n    if eunis_level == 2:\n        return level2_datacube"
              },
              "result": true
            }
          }
        }
      }
    },
    "saveresult1": {
      "process_id": "save_result",
      "arguments": {
        "data": {
          "from_node": "reducedimension1"
        },
        "format": "GTiff",
        "options": {}
      }
    },
    "textconcat1": {
      "process_id": "text_concat",
      "arguments": {
        "data": [
          {
            "from_parameter": "digitalId"
          },
          {
            "from_parameter": "scenarioId"
          }
        ],
        "separator": "/"
      }
    },
    "exportworkspace1": {
      "process_id": "export_workspace",
      "arguments": {
        "data": {
          "from_node": "saveresult1"
        },
        "merge": {
          "from_node": "textconcat1"
        },
        "workspace": "esa-weed-workspace"
      },
      "result": true
    }
  },
  "id": "udp_inference_module_alpha1",
  "summary": "Generates the alpha 1 inference result based on inputs.Returns a Single Tiff with Max Probability of the EUNIS Habitat Class",
  "description": "Inference for the habitat maps for the alpha1 release.",
  "default_job_options": {
    "driver-memory": "1000m",
    "driver-memoryOverhead": "1000m",
    "driver-cores": "1",
    "executor-memory": "1500m",
    "executor-memoryOverhead": "1500m",
    "executor-cores": "1",
    "max-executors": "20",
    "soft-errors": "true",
    "python-memory": "4000m",
    "udf-dependency-archives": [
      "https://s3.waw3-1.cloudferro.com/swift/v1/project_dependencies/onnx_dependencies_1.16.3.zip#onnx_deps"
    ]
  },
  "parameters": [
    {
      "name": "bbox",
      "description": "Limits the data to process to the specified bounding box or polygons.\n\nFor raster data, the process loads the pixel into the data cube if the point\n\n    at the pixel center intersects with the bounding box or any of the polygons\n(as defined in the Simple Features standard by the OGC).\n\nFor vector data, \n    the process loads the geometry into the data cube if the geometry\nis fully within the bounding box or any of the polygons (as defined in the\nSimple \n    Features standard by the OGC). Empty geometries may only be in the\ndata cube if no spatial extent has been provided.\n\nEmpty geometries are ignored.\n\nSet this parameter to null to set no limit for the spatial extent.",
      "schema": {
        "title": "Bounding Box",
        "type": "object",
        "subtype": "bounding-box",
        "required": [
          "west",
          "south",
          "east",
          "north"
        ],
        "properties": {
          "west": {
            "description": "West (lower left corner, coordinate axis 1).",
            "type": "number"
          },
          "south": {
            "description": "South (lower left corner, coordinate axis 2).",
            "type": "number"
          },
          "east": {
            "description": "East (upper right corner, coordinate axis 1).",
            "type": "number"
          },
          "north": {
            "description": "North (upper right corner, coordinate axis 2).",
            "type": "number"
          },
          "crs": {
            "description": "Coordinate reference system of the extent, specified as as [EPSG code](http://www.epsg-registry.org/) or [WKT2 CRS string](http://docs.opengeospatial.org/is/18-010r7/18-010r7.html).",
            "anyOf": [
              {
                "title": "EPSG Code",
                "type": "integer",
                "subtype": "epsg-code",
                "minimum": 1000,
                "examples": [
                  3035
                ]
              },
              {
                "title": "WKT2",
                "type": "string",
                "subtype": "wkt2-definition"
              }
            ],
            "default": 3035
          }
        }
      }
    },
    {
      "name": "year",
      "description": "The year for which to generate the habitat map. (default: 2021)",
      "schema": {
        "type": "integer"
      },
      "default": 2021,
      "optional": true
    },
    {
      "name": "digitalId",
      "description": "Digital ID of client",
      "schema": {
        "type": "string"
      }
    },
    {
      "name": "scenarioId",
      "description": "Id of the scenario/session",
      "schema": {
        "type": "string"
      }
    },
    {
      "name": "area_name",
      "description": "Name of the AOI",
      "schema": {
        "type": "string"
      }
    },
    {
      "name": "target_epsg",
      "description": "The desired output projection system, which is EPSG:3035 by default.",
      "schema": {
        "type": "integer"
      },
      "default": 3035,
      "optional": true
    },
    {
      "name": "target_res",
      "description": "The desired resolution, specified in units of the projection system, which is meters by default.",
      "schema": {
        "type": "number"
      },
      "default": 10.0,
      "optional": true
    },
    {
      "name": "eunis_level",
      "description": "EUNIS Level",
      "schema": {
        "type": "integer"
      },
      "default": 1,
      "optional": true
    }
  ]
}
